{"cells":[{"cell_type":"markdown","metadata":{"id":"I94g5POJKa3l"},"source":["# Tutorial for the neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions"]},{"cell_type":"markdown","metadata":{"id":"KzYa3rI5Ka3n"},"source":["This notebook provides a tutorial for our paper:  [Horikawa, Cowen, Keltner, and Kamitani (2020) The neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions. iScience](https://www.cell.com/iscience/fulltext/S2589-0042(20)30245-5?rss=yes).\n","\n","\n","このチュートリアルでは， 下記の github リポジトリで decoding および encoding analysis を行うための手続きを確認していきます．\n","\n","- [KamitaniLab/EmotionVideoNeuralRepresentationPython](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentationPython)\n","\n","Decoding および encoding のアルゴリズムの詳細を学びたい場合は，下記の別のリポジトリを参照してください．\n","\n","- [KamitaniLab/feature-decoding](https://github.com/KamitaniLab/feature-decoding)\n","- [KamitaniLab/feature-encoding](https://github.com/KamitaniLab/feature-encoding)\n","\n","\n","なお，ここでの計算はあくまでチュートリアル向けであり， 上記のpaperにある全ての計算は行っていないことに注意してください．paperの条件で計算を行う場合は，チュートリアル用のconfig fileではなく解析用のconfig fileを使用して下さい．"]},{"cell_type":"markdown","metadata":{"id":"urv_H5UfKa3n"},"source":["## Environment setup\n","このチュートリアルはGoogle colabでも実行でき，基本的にはGoogle colabのコマンドおよびマジックコマンドで手続きが示されています．\n","\n","** ローカルマシン上で実行する場合，このSetup セクションのノートブックセルは全て実行する必要はありません．** セル上に示されているコマンドライン操作を，各マシンのCUI上で実行してください．"]},{"cell_type":"markdown","metadata":{"id":"C9qXeCPhQYsR"},"source":["### ディレクトリ構成の準備\n","下記のgithubリポジトリをcloneしてください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1046,"status":"ok","timestamp":1723883056495,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"WXgvITX-QDpC","outputId":"d1f005e3-df51-4557-c318-fc3a68d774fe"},"outputs":[],"source":["# ローカルマシンで実行している場合，httpsがセキュリティ的に実行できない場合があります．その場合は，以下を clone 元として使用してください．\n","# git@github.com:KamitaniLab/EmotionVideoNeuralRepresentationPython.git \n","!git clone https://github.com/KamitaniLab/EmotionVideoNeuralRepresentationPython.git\n","%cd EmotionVideoNeuralRepresentationPython\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"xXdtDDA8QdOM"},"source":["ディレクトリEmotionVideoNeuralRepresentationPythonの下に，さらに下記の2つのgithubリポジトリをcloneしてください．\n","これらのスクリプトを使用することが前提になっています．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIvUmxA6QV7r"},"outputs":[],"source":["# ローカルマシンで実行している場合，httpsがセキュリティ的に実行できない場合があります．その場合は，以下を clone 元として使用してください．\n","# git@github.com:KamitaniLab/feature-decoding.git\n","# git@github.com:KamitaniLab/feature-encoding.git\n","!git clone https://github.com/KamitaniLab/feature-decoding.git\n","!git clone https://github.com/KamitaniLab/feature-encoding.git"]},{"cell_type":"markdown","metadata":{"id":"Z4kYQdF2Qk1D"},"source":["ディレクトリ構成が下記のようになっていることを確認してください．\n","\n","```\n","EmotionVideoNeuralRepresentationPython (current directory)\n","├── README.md\n","├── config/\n","├── data/\n","├── env.yaml\n","├── feature-decoding/\n","├── feature-encoding/\n","├── requirements.txt\n","└── src/\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwdboIroRGis"},"outputs":[],"source":["!pwd\n","!ls -1"]},{"cell_type":"markdown","metadata":{"id":"WIFNArgdRYAt"},"source":["### 必要なライブラリのインストールと設定の修正\n","EmotionVideoNeuralRepresentationPython 内の環境設定ファイルを用いて，環境の構築を行います．これにより，追加でcloneした2つのリポジトリも動作する環境が構築されます．\n","\n","ローカルマシンで実行している場合は，下記のセルを実行する代わりに， 下記のコマンドを実行し， conda 環境を作成することを推奨します． \n","```\n","$ conda env create --name <your environment name> -f env.yaml\n","$ activate <your environment name>\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41215,"status":"ok","timestamp":1723886417450,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"pcpmug-ORbnw","outputId":"1fadc147-775f-4598-9ee3-4af3b4652737"},"outputs":[],"source":["!pip install -r ./requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":["[pycortex](https://github.com/gallantlab/pycortex)を使用した皮質マップの可視化を行う場合，[Inkscape](https://wiki.inkscape.org/wiki/Installing_Inkscape)をインストールしてください．\n","Google colabの場合は下記のセルを実行し，ローカルマシンで実行している場合は使用している環境に合わせて適宜インストールしてください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!sudo apt-get install -y inkscape"]},{"cell_type":"markdown","metadata":{"id":"XJn0Qz5gnoiV"},"source":["pycortex での可視化を行う場合は， 設定ファイルを書き換えます．このチュートリアルをローカルマシンで実行している場合も，同様に書き換えてください．\n","\n","`~/.config/pycortex/options.cfg` の `[base]` セクションの `filestore` のパスを `data/pycortex` 以下に修正します．もし，誤ってすでに `import cortex` してしまった場合は，ランタイムを再起動してください．"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1458,"status":"ok","timestamp":1723869493343,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"LzqzkxbrnwDe","outputId":"b6bd092a-8f04-4c03-ecbb-152e9dc5b43e"},"outputs":[],"source":["!cat  ~/.config/pycortex/options.cfg | grep filestore\n","!cp ~/.config/pycortex/options.cfg ~/.config/pycortex/backup_options.cfg\n","!sed -i 's/filestore = \\/usr\\/share\\/pycortex\\/db/filestore = .\\/data\\/pycortex\\//' ~/.config/pycortex/options.cfg\n","!cat  ~/.config/pycortex/options.cfg | grep filestore\n"]},{"cell_type":"markdown","metadata":{"id":"gINVPAW7Rb_t"},"source":["### 必要なデータのダウンロード\n","\n","dataディレクトリの下に移動し， download.pyを実行して下さい．\n","download.py はダウンロード対象のデータを引数で指定することができます．今回はデモに使用するファイルのみをダウンロードします．\n","\n","- demo_fmri: Subject1のfMRIデータ\n","- demo_pycortex: Subject1のpycotexデータ\n","- feautres: 全てのemotional scores\n","\n","終了後は， `data`を抜けて，`EmotionVideoNeuralRepresentationPython`の直下に移動してください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ory3PBLzRcMA"},"outputs":[],"source":["%cd data\n","!python download.py demo_fmri\n","!python download.py demo_pycortex\n","!python download.py features\n","%cd ../"]},{"cell_type":"markdown","metadata":{"id":"3_AuaLnMSncK"},"source":["これで準備は完了です．\n"]},{"cell_type":"markdown","metadata":{"id":"e2wQvlwjbhEj"},"source":["## Tutorial for encoding analysis\n","\n","このセクションではencoding analysisの手続きについて説明します．このリポジトリでは，下記の手順でencodingを実行してきます．さらに次のセクションで説明するdecoding analysisについてもほぼ同じ手順で実行できます．\n","1. Encoding 設定ファイルを確認する\n","2. Encoderのtrainingを行う\n","3. Encoding prediction を行う\n","4. Encoding prediction の performance の計算\n","5. Encoding performance の可視化 \n"]},{"cell_type":"markdown","metadata":{"id":"pcQ29wf2SwDG"},"source":["\n","### Encoding 設定ファイルの確認\n","\n","まずは必要なモジュールをimportしてください"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"elapsed":462,"status":"error","timestamp":1723886828688,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"IJjLJeI2qRI8","outputId":"3285249c-33bf-4d2b-ca87-a17e04365f03"},"outputs":[],"source":["import os\n","import sys\n","from pathlib import Path\n","from hydra.experimental import initialize_config_dir, compose\n","from pathlib import Path\n","from omegaconf import OmegaConf, DictConfig\n"]},{"cell_type":"markdown","metadata":{"id":"nQyM2Cc4qSoA"},"source":["今回の計算対象の確認を行います．\n","\n","本稿では， encoding anlaysis として， emotional category (34 category) および affective dimension (14 dimension) の score を用いて，各被験者の皮質脳活動を推定しています．\n","\n","実際には5人の被験者に対して，それぞれ6-fold crossvalidationの計算を行っています．しかし，すべての計算を行うと時間がかかるため，ここでは省略のため，下記の条件に絞り込みます．\n","\n","1. 5subjectのうち，1subjectの計算のみを行う\n","2. semantic scoreおよびvision scoreの計算を行わず，category score, dimension scoreの計算のみを行う\n","2. 6-fold計算のうち，1-foldの計算のみを行う\n","3. fold計算中に，さらに入れ子のfold計算を行い，パラメータの最適化を行うフェーズが存在しない （これは現在のPythonコードリポジトリでは実装されていない機能です． オリジナルの[Matlabコードのリポジトリ](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentation/tree/master?tab=readme-ov-file)をご参照ください．）\n","\n","それでは，今回使用する tutorial 用の config ファイル `./config/tutorial_encoding_emotion_score_cv_paper2020.yaml` を確認しましょう．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":509,"status":"error","timestamp":1723864898176,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"xGH0ATRiTzAJ","outputId":"1910f916-d0db-46db-8c80-4cb8aee0d6c3"},"outputs":[],"source":["# ここで変数状態にまで持っていく\n","\n","config_file = \"./config/tutorial_encoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg = compose(config_name=config_name)    \n","\n","\n","# Convert settings\n","analysis_name = 'cv_train_encoder_fastl2lir-tutorial_encoding_emotion_score_cv_paper2020'\n","fmri = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg[\"encoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"encoder\"][\"fmri\"][\"rois\"]\n","}\n","label_key = cfg[\"encoder\"][\"fmri\"][\"label_key\"]\n","feature_dir_list = cfg[\"encoder\"][\"features\"][\"paths\"]\n","features = [feat[\"name\"] for feat in cfg[\"encoder\"][\"features\"][\"layers\"]]\n","num_unit = {\n","    layer[\"name\"]: layer[\"num\"]\n","    for layer in cfg[\"encoder\"][\"features\"][\"layers\"]\n","}\n","feature_index_file = cfg.encoder.features.get(\"index_file\", None)\n","encoder_dir = cfg[\"encoder\"][\"path\"]\n","encoded_fmri_dir = cfg[\"encoded_fmri\"][\"path\"]\n","cv_folds = cfg.cv.get(\"folds\", None)\n","cv_exclusive = cfg.cv.get(\"exclusive_key\", None)\n","cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]\n","excluded_labels = cfg.encoded_fmri.fmri.get(\"exclude_labels\", [])\n","average_sample = cfg.encoded_fmri.parameters.get(\"average_sample\", True)\n","evaluation_rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"evaluation\"][\"fmri\"][\"rois\"]\n","}\n","\n","print(\"====== Encoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri.keys())))\n","print(\"ROIs:     {}\".format(list(rois.keys())))\n","print(\"Features: {}\".format(features))\n","print(\"Folds:    {}\".format(cv_labels))\n","print(\"\")\n","print(\"fMRI data dir:           {}\".format(os.path.split(list(fmri.values())[0][0])[0]))\n","print(\"Feature data dir:        {}\".format(feature_dir_list[0]))\n","print(\"\")\n","print(\"Save encoder dir:        {}\".format(encoder_dir))\n","print(\"Save encoded singal dir: {}\".format(encoded_fmri_dir))\n","print(\"===============================\")\n"]},{"cell_type":"markdown","metadata":{},"source":["上記の条件で計算を行っていきます．実際の解析で用いられている条件についても，下記のセルを実行すると確認できます．\n","\n","config file: `./config/encoding_emotion_score_cv_paper2020.yaml` \n","\n","すべての条件を計算する場合は，計算量の増加により計算時間がスケールすることに注意してください．（ある程度のスペックのあるローカルマシン環境で計算することを推奨します．）"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Original encoding analysis settings\n","\n","config_file = \"./config/encoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg2 = compose(config_name=config_name)    \n","\n","# Convert settings\n","fmri2 = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg2[\"encoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois2 = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg2[\"encoder\"][\"fmri\"][\"rois\"]\n","}\n","feature_dir_list2 = cfg2[\"encoder\"][\"features\"][\"paths\"]\n","features2 = [feat[\"name\"] for feat in cfg2[\"encoder\"][\"features\"][\"layers\"]]\n","encoder_dir2 = cfg2[\"encoder\"][\"path\"]\n","encoded_fmri_dir2 = cfg2[\"encoded_fmri\"][\"path\"]\n","cv_folds2 = cfg2.cv.get(\"folds\", None)\n","cv_labels2 = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds2))]\n","\n","from pprint import pprint\n","print(\"====== Encoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri2.keys())))\n","print(\"ROIs:     {}\".format(list(rois2.keys())))\n","print(\"Features: {}\".format(features2))\n","print(\"Folds:    {}\".format(cv_labels2))\n","print(\"\")\n","print(\"fMRI data dir:           {}\".format(os.path.split(list(fmri2.values())[0][0])[0]))\n","print(\"Feature data dir:        {}\".format(feature_dir_list2[0]))\n","print(\"\")\n","print(\"Save encoder dir:        {}\".format(encoder_dir2))\n","print(\"Save encoded singal dir: {}\".format(encoded_fmri_dir2))\n","print(\"===============================\")"]},{"cell_type":"markdown","metadata":{},"source":["今回の計算対象の確認を行います．\n","\n","本稿では， encoding anlaysis として， emotional category (34 category) および affective dimension (14 dimension) の score を用いて，各被験者の皮質脳活動を推定しています．\n","\n","実際には5人の被験者に対して，それぞれ6-fold crossvalidationの計算を行っています．しかし，すべての計算を行うと時間がかかるため，ここでは省略のため，下記の条件に絞り込みます．\n","\n","1. 5subjectのうち，1subjectの計算のみを行う\n","2. semantic scoreおよびvision scoreの計算を行わず，category score, dimension scoreの計算のみを行う\n","2. 6-fold計算のうち，1-foldの計算のみを行う\n","3. fold計算中に，さらに入れ子のfold計算を行い，パラメータの最適化を行うフェーズが存在しない （これは現在のPythonコードリポジトリでは実装されていない機能です． オリジナルの[Matlabコードのリポジトリ](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentation/tree/master?tab=readme-ov-file)をご参照ください．）\n","\n","それでは，今回使用する tutorial 用の config ファイル `./config/tutorial_encoding_emotion_score_cv_paper2020.yaml` を確認しましょう．\n"]},{"cell_type":"markdown","metadata":{"id":"v41ySd8mZLFZ"},"source":["それでは，上記のtutorial のセッティングで計算を行っていきます．"]},{"cell_type":"markdown","metadata":{"id":"F8WhQB3qVKEy"},"source":["### Encoder training を行う (~5分)\n","Encoderのtrainingを行います． 実行時のカレントディレクトリが， `EmotionVideoNeuralRepresentationPython` の直下であることを確認してください．\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます\n","\n","`$ python feature-encoding/cv_train_encoder_fastl2lir.py config/encoding_emotion_score_cv_paper2020.yaml`\n","\n","この計算は，異なるマシン上で並行して実行することが可能です（カレントディレクトリ，および保存先ディレクトリをマシン間で共有している必要があります）．\n","特に，全ての条件を計算する場合，複数マシンでの並列実行を推奨します．\n","\n","ここでは，　上記のスクリプト `cv_train_encoder_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vT0fJY_V815"},"outputs":[],"source":["sys.path.insert(0, \"feature-encoding\")\n","from cv_train_encoder_fastl2lir import featenc_cv_fastl2lir_train\n","\n","featenc_cv_fastl2lir_train(\n","    fmri,\n","    feature_dir_list,\n","    output_dir=encoder_dir,\n","    rois=rois,\n","    label_key=label_key,\n","    layers=features,\n","    num_unit=num_unit,\n","    feature_index_file=feature_index_file,\n","    alpha=cfg[\"encoder\"][\"parameters\"][\"alpha\"],\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    analysis_name=analysis_name\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"4iBVOysvUUfF"},"source":["下記のディレクトリに訓練済みencoderモデルが出力されます．\n","\n","`<Save encoder dir>/<feature>/<subject>/<ROI>/<Fold>/model`\n","\n","- `W.mat`: Weight of encoder model\n","- `b.mat`: Bias of encoder model\n","- `x_mean.mat`, `x_norm.mat`: Feature normalization parameters. \n","- `y_mean.mat`, `y_norm.mat`: Voxel normalization parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1723881101532,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"X8Gs2QDqUQbX","outputId":"0a476a1d-2f89-4e42-efc2-5dbbac6eb596"},"outputs":[],"source":["!ls ./data/feature_encoders/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat\n","!ls ./data/feature_encoders/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/WholeBrain/cv-fold1/model\n"]},{"cell_type":"markdown","metadata":{"id":"w51vRs0YWNhG"},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．並列計算した場合は，必ず実行してください．\n","\n","`$ python src/check_training_prediction.py config/encoding_emotion_score_cv_paper2020.yaml --override +wash=train`\n","\n","ここでは，　上記のスクリプト `` で呼び出されるメイン関数を直接呼び出し，実行します．\n","下記が出力されれば問題ありません．\n","\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","```\n","\n","もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5JsCc_hWYH7"},"outputs":[],"source":["sys.path.insert(0, \"src\")\n","from check_training_prediction import check_training_prediction\n","\n","target = \"train\"\n","analysis_type = \"encoding\"\n","\n","check_training_prediction(\n","    encoder_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"JSYQPeX9WY34"},"source":["これで encoder の training は完了です．\n","Encoding(training/prediction)の詳細な手続きを知りたい場合は，　feature-encoding リポジトリ内の handson 02_encoding.ipynb を参照してください．"]},{"cell_type":"markdown","metadata":{"id":"b-t8chEZYKdW"},"source":["### Encoding prediction を行う（~5分）\n","前セクションでtrainingしたencoderを用いて，predictionを行います．実行時のカレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください．\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の prediction が開始されます\n","\n","`$ python src/cv_predict_fmri_fastl2lir.py config/encoding_emotion_score_cv_paper2020.yaml`\n","\n","この計算も，並列実行することが可能です．\n","\n","ここでは，　上記のスクリプト `cv_predict_fmri_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tDd5lDkYvWr"},"outputs":[],"source":["sys.path.insert(0, \"feature-encoding\")\n","from cv_predict_fmri_fastl2lir import featenc_cv_fastl2lir_predict\n","\n","featenc_cv_fastl2lir_predict(\n","    feature_dir_list,\n","    fmri,\n","    encoder_dir,\n","    output_dir=encoded_fmri_dir,\n","    subjects=list(fmri.keys()),\n","    rois=rois,\n","    label_key=label_key,\n","    layers=features,\n","    feature_index_file=feature_index_file,\n","    excluded_labels=excluded_labels,\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    analysis_name=analysis_name\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5IpiS8BXYvxP"},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．\n","\n","`$ python src/check_training_prediction.py config/encoding_emotion_score_cv_paper2020.yaml --override +wash=predict`\n","\n","ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します．下記の2つの結果が出力されることを確認してください．\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","\n","==========\n","All predicted files exist.\n","==========\n","```\n","もし，未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":5,"status":"error","timestamp":1723865579149,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"FVy2vASBYyMQ","outputId":"f9e9f92d-8c12-4fc4-b847-982faab1532c"},"outputs":[],"source":["sys.path.insert(0, \"src\")\n","from check_training_prediction import check_training_prediction\n","\n","target = \"predict\"\n","analysis_type = \"encoding\"\n","\n","check_training_prediction(\n","    encoded_fmri_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"-mL8v4zVZSmA"},"source":["これで encoder の prediction は完了です．\n","Encoding(training/prediction)の詳細な手続きを知りたい場合は，　feature-encoding リポジトリ内の handson 02_encoding.ipynb を参照してください．"]},{"cell_type":"markdown","metadata":{"id":"4S2L9SN0ZXOt"},"source":["### Encoding prediction の performance の計算 （~5分）\n","\n","Encoding結果の評価を行います．具体的には，前セクションで推定されたfMRI signalと，実際に観測されたfMRI singalについて，voxelごとにcorrelation coefficientを計算します．\n","実行時のカレントディレクトリは， EmotionVideoNeuralRepresentationPython の直下としてください．\n","\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます\n","\n","`$ python src/cv_evaluate_predicted_fmri.py config/encoding_emotion_score_cv_paper2020.yaml `\n","\n","この計算は，並列実行することが可能です．デモでは不要ですが，全ての条件を計算する場合，複数台のマシンでの並列実行を推奨します．\n","\n","ここでは，　上記のスクリプト `cv_evaluate_predicted_fmri.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGO8YQypY2Fz"},"outputs":[],"source":["from cv_evaluate_predicted_fmri import cv_evaluate_predicted_fmri\n","\n","cv_evaluate_predicted_fmri(\n","    encoded_fmri_dir,\n","    encoder_dir,\n","    fmri,\n","    rois,\n","    evaluation_rois,\n","    features,\n","    cv_folds,\n","    label_key,\n","    output_file_pooled=os.path.join(encoded_fmri_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(encoded_fmri_dir, 'evaluation_fold.db'),\n","    average_sample=average_sample,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"Q6prb0qDZyjG"},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．\n","\n","`$ python src/check_evaluate_database.py config/encoding_emotion_score_cv_paper2020.yaml `\n","\n","ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します． `All conditions are finished` が二度出力されていることを確認してください．一つは各foldの計算が完了していることを示しており，もう一つはfoldの計算結果を平均したpooledの計算が完了していることを示しています．\n","```\n","==========\n","All conditions are finished.\n","==========\n","```\n","もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIoxFmPNZ1hh"},"outputs":[],"source":["sys.path.insert(0, \"src\")\n","from check_evaluate_database import check_evaluate_database\n","\n","analysis_type = \"encoding\"\n","\n","check_evaluate_database(\n","    encoded_fmri_dir,\n","    analysis_type,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n","    output_file_pooled=os.path.join(encoded_fmri_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(encoded_fmri_dir, 'evaluation_fold.db'),\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"VS9mLkKuZ2bW"},"source":["これで encoding analysis の全てのステップが完了しました．\n","Encoding(training/prediction/evaluation)の詳細な手続きを知りたい場合は，　[feature-encoding](https://github.com/KamitaniLab/feature-encoding) リポジトリ内の handson 02_encoding.ipynb を参照してください．"]},{"cell_type":"markdown","metadata":{"id":"INgiJvxAYywd"},"source":["### Encoding performance の可視化 （~5分）\n","\n","ここでは前セクションで計算された correlation coefficient を，皮質上にマップして結果を確認します．\n"]},{"cell_type":"markdown","metadata":{"id":"VFmWEew6pB1d"},"source":["Pycortexを使用する場合， `pycortex` の `filestore` が `./data/pycortex` であること，および`INKSCAPE_VERSION`が`None`ではないことを確認してください．（条件が満たされていない場合，ランタイムを再起動させてください．）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgCOcw5MpGKn"},"outputs":[],"source":["import cortex\n","print('pycortex db:', cortex.config['basic']['filestore'])\n","print('INKSCAPE_VERSION:', cortex.testing_utils.INKSCAPE_VERSION)\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","下記のセルを実行して，結果の可視化を行ってください．各被験者のemotion scoreごとのencoding accuracyが表示されます．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import bdpy\n","from bdpy.dataform import SQLite3KeyValueStore\n","from itertools import product\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","pycortex_align = 'voxel2mm_fmriprep'\n","pycortex_overlay_filepath = './data/pycortex/{}/overlays.svg'\n","metric = \"profile_correlation\"\n","vmin = 0.1\n","vmax = 0.4\n","\n","for subject, roi, evaluation_roi in product(fmri.keys(), rois.keys(), evaluation_rois.keys()):\n","    # Load Bdata and voxel information\n","    bdata = bdpy.BData(fmri[subject][0])\n","    _, encoding_roi_selector = bdata.select(roi, return_index = True)\n","    _, visualize_roi_selector = bdata.select(evaluation_roi, return_index = True)\n","    voxel_selector = visualize_roi_selector[encoding_roi_selector]\n","    \n","    # Load accuracy\n","    evaluation_filepath = os.path.join(encoded_fmri_dir, \"evaluation.db\")\n","    db = SQLite3KeyValueStore(evaluation_filepath)\n","        \n","    for feature in features:\n","        print(\"Feature:\", feature)\n","\n","        # Create emty volume\n","        # Colormap (cmap) for visualization is set here. Available cmap is same as Matplotlib \n","        # (See document for matplotlib.pylab.colormaps()).\n","        voxel_vol = cortex.Volume.empty(subject, \n","                                        pycortex_align, \n","                                        value=np.nan, cmap='hot')                \n","\n","        # Apply voxel selector to voxel_i/j/k array\n","        voxel_ijk = np.vstack([\n","            bdata.get_metadata('voxel_i')[visualize_roi_selector].astype('int32'),\n","            bdata.get_metadata('voxel_j')[visualize_roi_selector].astype('int32'),\n","            bdata.get_metadata('voxel_k')[visualize_roi_selector].astype('int32'),\n","        ]).astype(np.int)\n","\n","        # Get each voxel accuracy\n","        acc = db.get(subject=subject, roi=roi, layer=feature, metric=metric)\n","        acc = acc[voxel_selector]\n","        acc[acc < vmin] = np.nan\n","        voxel_vol.data[ voxel_ijk[2], voxel_ijk[1], voxel_ijk[0] ] = acc\n","\n","        # Set vmax and vmin for visualization                    \n","        voxel_vol.vmax = vmax\n","        voxel_vol.vmin  = 0\n","\n","        # Set text\n","        text_posi_x = 0.5\n","        text_posi_y = 0.9\n","        show_text = feature\n","        \n","        # Show\n","        fig = cortex.quickshow( \n","                    voxel_vol, with_colorbar=True, with_curvature=True, \n","                    overlay_file=pycortex_overlay_filepath.format(subject),\n","                    labelsize=50, linewidth=3,\n","            )\n","        fig.text(text_posi_x, text_posi_y, show_text, \n","                    horizontalalignment='center', verticalalignment='center', size=50)\n","        plt.show()\n","        "]},{"cell_type":"markdown","metadata":{},"source":["以上でencoding analysisのtutorialは終了です．\n"]},{"cell_type":"markdown","metadata":{"id":"4yoa2D8wYukr"},"source":["## Tutorial for decoding analysis"]},{"cell_type":"markdown","metadata":{"id":"Qy4Ni63hYunQ"},"source":["### Decoding 設定ファイルの確認"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ここで変数状態にまで持っていく\n","\n","config_file = \"./config/tutorial_decoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg = compose(config_name=config_name)\n","    \n","training_fmri = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg[\"decoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","num_voxel = {\n","    roi[\"name\"]: roi[\"num\"]\n","    for roi in cfg[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","label_key = cfg[\"decoder\"][\"fmri\"][\"label_key\"]\n","training_features = cfg[\"decoder\"][\"features\"][\"paths\"]\n","features = cfg[\"decoder\"][\"features\"][\"layers\"]\n","feature_index_file = cfg.decoder.features.get(\"index_file\", None)\n","decoder_dir = cfg[\"decoder\"][\"path\"]\n","cv_folds = cfg.cv.get(\"folds\", None)\n","cv_exclusive = cfg.cv.get(\"exclusive_key\", None)\n","\n","if 'name' in cv_folds[0]:\n","    cv_labels = ['cv-{}'.format(cv['name']) for cv in cv_folds]\n","else:\n","    cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]\n","\n","from pprint import pprint\n","print(\"====== Encoding settings ======\")\n","print(\"Subjects(fMRI data):\", [subject[\"name\"] for subject in cfg[\"decoder\"][\"fmri\"][\"subjects\"]])\n","print(\"ROIs:\", [roi[\"name\"] for roi in cfg[\"decoder\"][\"fmri\"][\"rois\"]])\n","print(\"Features:\", cfg[\"decoder\"][\"features\"][\"layers\"])\n","print(\"Folds:\", cv_folds)\n","print(\"\")\n"]},{"cell_type":"markdown","metadata":{"id":"6YpLsBmGYupW"},"source":["### Decoder trainingを行う（~5分）"]},{"cell_type":"markdown","metadata":{"id":"qqikav6XYurs"},"source":[]},{"cell_type":"markdown","metadata":{"id":"44DPFh87YuuD"},"source":[]},{"cell_type":"markdown","metadata":{"id":"kQOP0fhVYuwY"},"source":[]},{"cell_type":"markdown","metadata":{"id":"hetHTuX_Yuy_"},"source":[]},{"cell_type":"markdown","metadata":{"id":"R6F7_d7JYu1W"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
