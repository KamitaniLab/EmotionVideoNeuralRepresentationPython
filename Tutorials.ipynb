{"cells":[{"cell_type":"markdown","metadata":{"id":"I94g5POJKa3l"},"source":["# Tutorial for the neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions"]},{"cell_type":"markdown","metadata":{"id":"KzYa3rI5Ka3n"},"source":["This notebook provides a tutorial for our paper:  [Horikawa, Cowen, Keltner, and Kamitani (2020) The neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions. iScience](https://www.cell.com/iscience/fulltext/S2589-0042\\(20\\)30245-5?rss=yes).\n","\n","\n","このチュートリアルでは， 下記の github リポジトリで decoding および encoding analysis を行うための手続きを確認していきます．\n","\n","- [KamitaniLab/EmotionVideoNeuralRepresentationPython](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentationPython)\n","\n","Decoding および encoding のアルゴリズムの詳細を学びたい場合は，下記の別のリポジトリを参照してください．\n","\n","- [KamitaniLab/feature-decoding](https://github.com/KamitaniLab/feature-decoding)\n","- [KamitaniLab/feature-encoding](https://github.com/KamitaniLab/feature-encoding)\n","\n","\n","なお，ここでの計算はあくまでチュートリアル向けであり， 上記のpaperにある全ての計算は行っていないことに注意してください．paperの条件で計算を行う場合は，チュートリアル用のconfig fileではなく解析用のconfig fileを使用して下さい．"]},{"cell_type":"markdown","metadata":{"id":"urv_H5UfKa3n"},"source":["## Environment setup（〜15分）\n","このチュートリアルはGoogle colabでも実行でき，基本的にはGoogle colabのコマンドおよびマジックコマンドで手続きが示されています．\n","\n","** ローカルマシン上で実行する場合，このSetup セクションのノートブックセルは全て実行する必要はありません．** セル上に示されているコマンドライン操作を，各マシンのCUI上で実行してください．"]},{"cell_type":"markdown","metadata":{"id":"C9qXeCPhQYsR"},"source":["### ディレクトリ構成の準備\n","下記のgithubリポジトリをcloneし，ディレクトリを移動してください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1046,"status":"ok","timestamp":1723883056495,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"WXgvITX-QDpC","outputId":"d1f005e3-df51-4557-c318-fc3a68d774fe"},"outputs":[],"source":["!git clone https://github.com/KamitaniLab/EmotionVideoNeuralRepresentationPython.git\n","# or git@github.com:KamitaniLab/EmotionVideoNeuralRepresentationPython.git \n","\n","%cd EmotionVideoNeuralRepresentationPython\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"xXdtDDA8QdOM"},"source":["ディレクトリ`EmotionVideoNeuralRepresentationPython`の下に，さらに下記の2つの github リポジトリを clone してください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIvUmxA6QV7r"},"outputs":[],"source":["!git clone https://github.com/KamitaniLab/feature-decoding.git\n","# or git@github.com:KamitaniLab/feature-decoding.git\n","!git clone https://github.com/KamitaniLab/feature-encoding.git\n","# or git@github.com:KamitaniLab/feature-encoding.git\n"]},{"cell_type":"markdown","metadata":{"id":"Z4kYQdF2Qk1D"},"source":["ディレクトリ構成が下記のようになっていることを確認してください．\n","\n","```\n","EmotionVideoNeuralRepresentationPython (current directory)\n","├── config/\n","├── data/\n","├── env.yaml\n","├── feature-decoding/\n","├── feature-encoding/\n","├── README.md\n","├── requirements.txt\n","├── src/\n","└── Tutorials.ipynb\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwdboIroRGis"},"outputs":[],"source":["!pwd\n","!ls -1"]},{"cell_type":"markdown","metadata":{"id":"WIFNArgdRYAt"},"source":["### 必要なライブラリのインストールと設定の修正\n","`EmotionVideoNeuralRepresentationPython` 内の環境設定ファイルを用いて，環境の構築を行います．これにより，追加でcloneした2つのリポジトリも動作する環境が構築されます．\n","\n","ローカルマシンで実行している場合は，下記のセルを実行する代わりに， 下記のコマンドを実行し， conda 環境を作成することを推奨します． \n","```\n","$ conda env create --name <your environment name> -f env.yaml\n","$ activate <your environment name>\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41215,"status":"ok","timestamp":1723886417450,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"pcpmug-ORbnw","outputId":"1fadc147-775f-4598-9ee3-4af3b4652737"},"outputs":[],"source":["!pip install -r ./requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":["[pycortex](https://github.com/gallantlab/pycortex)を使用した皮質マップの可視化を行う場合，`pycortex`と[Inkscape](https://wiki.inkscape.org/wiki/Installing_Inkscape)をインストールしてください．\n","Google colabの場合は下記のセルを実行してください．\n","ローカルマシンで実行している場合は使用している環境に合わせて適宜インストールしてください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install pycortex\n","!sudo apt-get install -y inkscape"]},{"cell_type":"markdown","metadata":{"id":"XJn0Qz5gnoiV"},"source":["`pycortex` での可視化を行う場合は， 設定ファイルを書き換えます．このチュートリアルをローカルマシンで実行している場合も，同様に書き換えてください．\n","\n","まず一度`cortex`パッケージのimportを行います．\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cortex"]},{"cell_type":"markdown","metadata":{},"source":["これにより `~/.config/pycortex/options.cfg` が生成されます．\n","この設定ファイルの `[base]` セクションの `filestore` のパスを `data/pycortex` 以下に修正します．"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1458,"status":"ok","timestamp":1723869493343,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"LzqzkxbrnwDe","outputId":"b6bd092a-8f04-4c03-ecbb-152e9dc5b43e"},"outputs":[],"source":["!cat  ~/.config/pycortex/options.cfg | grep filestore\n","!cp ~/.config/pycortex/options.cfg ~/.config/pycortex/backup_options.cfg\n","!sed -i 's/filestore = \\/usr\\/share\\/pycortex\\/db/filestore = .\\/data\\/pycortex\\//' ~/.config/pycortex/options.cfg\n","!cat  ~/.config/pycortex/options.cfg | grep filestore\n"]},{"cell_type":"markdown","metadata":{},"source":["完了後，**必ずランタイムを再起動してください．** `pycortex`は import 時に上記の設定の読み込みを行います．\n","\n","再起動後は再び　`EmotionVideoNeuralRepresentationPython`  の直下に移動してください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd EmotionVideoNeuralRepresentationPython"]},{"cell_type":"markdown","metadata":{"id":"gINVPAW7Rb_t"},"source":["### 必要なデータのダウンロード\n","\n","`data` ディレクトリの下に移動し， `download.py` を実行して下さい．\n","`download.py` はダウンロード対象のデータを引数で指定することができます．今回はデモに使用するファイルのみをダウンロードします．\n","\n","- demo_fmri: Subject1のfMRIデータ\n","- demo_pycortex: Subject1のpycotexデータ\n","- feautres: 全てのemotional scores\n","\n","終了後は， `data`を抜けて，`EmotionVideoNeuralRepresentationPython`の直下に移動してください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ory3PBLzRcMA"},"outputs":[],"source":["%cd data\n","!python download.py demo_fmri\n","!python download.py demo_pycortex\n","!python download.py features\n","%cd ../"]},{"cell_type":"markdown","metadata":{"id":"3_AuaLnMSncK"},"source":["これで準備は完了です．\n"]},{"cell_type":"markdown","metadata":{"id":"e2wQvlwjbhEj"},"source":["## Tutorial for encoding analysis\n","\n","このセクションではencoding analysisの手続きについて説明します．このリポジトリでは，下記の手順でencodingを実行してきます．さらに次のセクションで説明するdecoding analysisについてもほぼ同じ手順で実行できます．\n","1. Encoding 設定ファイルを確認する\n","2. Encoderのtrainingを行う\n","3. Encoding prediction を行う\n","4. Encoding prediction の performance の計算\n","5. Encoding performance の可視化 \n"]},{"cell_type":"markdown","metadata":{"id":"pcQ29wf2SwDG"},"source":["\n","### Encoding 設定ファイルの確認\n","\n","まずは `EmotionVideoNeuralRepresentationPython` の直下にいることを確認してください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd EmotionVideoNeuralRepresentationPython\n","!pwd"]},{"cell_type":"markdown","metadata":{},"source":["次に必要なモジュールをimportしてください"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"elapsed":462,"status":"error","timestamp":1723886828688,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"IJjLJeI2qRI8","outputId":"3285249c-33bf-4d2b-ca87-a17e04365f03"},"outputs":[],"source":["import os\n","import sys\n","from itertools import product\n","from pathlib import Path\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from hydra.experimental import initialize_config_dir, compose\n","import cortex\n","\n","import bdpy\n","from bdpy.dataform import SQLite3KeyValueStore\n","\n","sys.path.insert(0, \"./src\")\n","sys.path.insert(0, \"./feature-encoding\")\n","sys.path.insert(0, \"./feature-decoding\")"]},{"cell_type":"markdown","metadata":{"id":"nQyM2Cc4qSoA"},"source":["それではこのtutorialにおけるencoding analysisの計算対象の確認を行います．\n","\n","本稿では， encoding anlaysis として， emotional category (34 category)，affective dimension (14 dimension)，semantic feature (73 feat), vision feature (1000 feat) の score を用いて，各被験者の脳活動を推定しています．\n","\n","実際には5人の被験者に対して，それぞれ6-fold crossvalidationの計算を行っています．しかし，すべての計算を行うと時間がかかるため，ここでは省略のため，下記の条件に絞り込みます．\n","\n","1. 5subjectのうち，1subjectの計算のみを行う\n","2. 皮質脳活動の計算のみを行う（subcortexについて計算しない）\n","3. category and dimension score の計算のみを行う（semantic and vision score の計算を行わない）\n","4. 6-fold計算のうち，1-foldの計算のみを行う\n","5. 交差検証のfold計算中に，さらに入れ子のfold計算を行い，パラメータの最適化を行うフェーズが存在しない （これは現在のPythonコードリポジトリでは実装されていない機能です． オリジナルの[Matlabコードのリポジトリ](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentation/tree/master?tab=readme-ov-file)をご参照ください．）\n","\n","それでは，今回使用する tutorial 用の config ファイル `./config/tutorial_encoding_emotion_score_cv_paper2020.yaml` を確認しましょう．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":509,"status":"error","timestamp":1723864898176,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"xGH0ATRiTzAJ","outputId":"1910f916-d0db-46db-8c80-4cb8aee0d6c3"},"outputs":[],"source":["# Load config data\n","config_file = \"./config/tutorial_encoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg = compose(config_name=config_name)    \n","\n","# Convert settings to variables\n","analysis_name = 'cv_train_encoder_fastl2lir-tutorial_encoding_emotion_score_cv_paper2020'\n","fmri = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg[\"encoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"encoder\"][\"fmri\"][\"rois\"]\n","}\n","label_key = cfg[\"encoder\"][\"fmri\"][\"label_key\"]\n","feature_dir_list = cfg[\"encoder\"][\"features\"][\"paths\"]\n","features = [feat[\"name\"] for feat in cfg[\"encoder\"][\"features\"][\"layers\"]]\n","num_unit = {\n","    layer[\"name\"]: layer[\"num\"]\n","    for layer in cfg[\"encoder\"][\"features\"][\"layers\"]\n","}\n","feature_index_file = cfg.encoder.features.get(\"index_file\", None)\n","encoder_dir = cfg[\"encoder\"][\"path\"]\n","encoded_fmri_dir = cfg[\"encoded_fmri\"][\"path\"]\n","cv_folds = cfg.cv.get(\"folds\", None)\n","cv_exclusive = cfg.cv.get(\"exclusive_key\", None)\n","cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]\n","excluded_labels = cfg.encoded_fmri.fmri.get(\"exclude_labels\", [])\n","average_sample = cfg.encoded_fmri.parameters.get(\"average_sample\", True)\n","evaluation_rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"evaluation\"][\"fmri\"][\"rois\"]\n","}\n","\n","# Show settings\n","print(\"====== Encoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri.keys())))\n","print(\"ROIs:     {}\".format(list(rois.keys())))\n","print(\"Features: {}\".format(features))\n","print(\"Folds:    {}\".format(cv_labels))\n","print(\"\")\n","print(\"fMRI data dir:           {}\".format(os.path.split(list(fmri.values())[0][0])[0]))\n","print(\"Feature data dir:        {}\".format(feature_dir_list[0]))\n","print(\"\")\n","print(\"Save encoder dir:        {}\".format(encoder_dir))\n","print(\"Save encoded singal dir: {}\".format(encoded_fmri_dir))\n","print(\"===============================\")\n"]},{"cell_type":"markdown","metadata":{},"source":["上記の条件で計算を行っていきます．実際の解析で用いられている設定（`./config/encoding_emotion_score_cv_paper2020.yaml`）についても，下記のセルを実行すると確認できます．\n","\n","すべての条件を計算する場合は，計算量の増加により計算時間がスケールすることに注意してください．（ある程度のスペックのあるローカルマシン環境で計算することを推奨します．）"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Original encoding analysis settings\n","\n","config_file = \"./config/encoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg2 = compose(config_name=config_name)    \n","\n","# Convert settings\n","fmri2 = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg2[\"encoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois2 = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg2[\"encoder\"][\"fmri\"][\"rois\"]\n","}\n","feature_dir_list2 = cfg2[\"encoder\"][\"features\"][\"paths\"]\n","features2 = [feat[\"name\"] for feat in cfg2[\"encoder\"][\"features\"][\"layers\"]]\n","encoder_dir2 = cfg2[\"encoder\"][\"path\"]\n","encoded_fmri_dir2 = cfg2[\"encoded_fmri\"][\"path\"]\n","cv_folds2 = cfg2.cv.get(\"folds\", None)\n","cv_labels2 = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds2))]\n","\n","from pprint import pprint\n","print(\"====== Encoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri2.keys())))\n","print(\"ROIs:     {}\".format(list(rois2.keys())))\n","print(\"Features: {}\".format(features2))\n","print(\"Folds:    {}\".format(cv_labels2))\n","print(\"\")\n","print(\"fMRI data dir:           {}\".format(os.path.split(list(fmri2.values())[0][0])[0]))\n","print(\"Feature data dir:        {}\".format(feature_dir_list2[0]))\n","print(\"\")\n","print(\"Save encoder dir:        {}\".format(encoder_dir2))\n","print(\"Save encoded singal dir: {}\".format(encoded_fmri_dir2))\n","print(\"===============================\")"]},{"cell_type":"markdown","metadata":{"id":"v41ySd8mZLFZ"},"source":["それでは，上記の tutorial のセッティングで計算を行っていきます．"]},{"cell_type":"markdown","metadata":{"id":"F8WhQB3qVKEy"},"source":["### Encoder training を行う (~5分)\n","Encoderのtrainingを行います． カレントディレクトリが， `EmotionVideoNeuralRepresentationPython` の直下であることを確認してください．（`!pwd`）\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます\n","\n","`$ python feature-encoding/cv_train_encoder_fastl2lir.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml`\n","\n","この計算は，異なるマシン上で並行して実行することが可能です（カレントディレクトリ，および保存先ディレクトリをマシン間で共有している必要があります）．\n","特に，全ての条件を計算する場合，複数マシンでの並列実行を推奨します．\n","\n","ここでは，　上記のスクリプト `cv_train_encoder_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vT0fJY_V815"},"outputs":[],"source":["from cv_train_encoder_fastl2lir import featenc_cv_fastl2lir_train\n","\n","featenc_cv_fastl2lir_train(\n","    fmri,\n","    feature_dir_list,\n","    output_dir=encoder_dir,\n","    rois=rois,\n","    label_key=label_key,\n","    layers=features,\n","    num_unit=num_unit,\n","    feature_index_file=feature_index_file,\n","    alpha=cfg[\"encoder\"][\"parameters\"][\"alpha\"],\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    analysis_name=analysis_name\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"4iBVOysvUUfF"},"source":["下記のディレクトリに訓練済みencoderモデルが出力されます．\n","\n","`<Save encoder dir>/<feature>/<subject>/<ROI>/<Fold>/model/`\n","\n","e.g.) `./data/feature_encoders/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/model/`\n","\n","- `W.mat`: Weight of encoder model\n","- `b.mat`: Bias of encoder model\n","- `x_mean.mat`, `x_norm.mat`: Feature normalization parameters. \n","- `y_mean.mat`, `y_norm.mat`: Voxel normalization parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1723881101532,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"X8Gs2QDqUQbX","outputId":"0a476a1d-2f89-4e42-efc2-5dbbac6eb596"},"outputs":[],"source":["!ls ./data/feature_encoders/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/model\n"]},{"cell_type":"markdown","metadata":{"id":"w51vRs0YWNhG"},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．実行途中でエラーが発生したり，並列計算したりした場合は，必ず実行してください．\n","\n","`$ python src/check_training_prediction.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml --override +wash=train`\n","\n","ここでは，　上記のスクリプト `` で呼び出されるメイン関数を直接呼び出し，実行します．\n","下記が出力されれば問題ありません．\n","\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","```\n","\n","もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5JsCc_hWYH7"},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"train\"\n","analysis_type = \"encoding\"\n","\n","check_training_prediction(\n","    encoder_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"JSYQPeX9WY34"},"source":["これで encoder の training は完了です．\n"]},{"cell_type":"markdown","metadata":{"id":"b-t8chEZYKdW"},"source":["### Encoding prediction を行う（~5分）\n","前セクションでtrainingしたencoder modelを用いて，predictionを行います．カレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください．\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の prediction が開始されます\n","\n","`$ python feature-encoding/cv_predict_fmri_fastl2lir.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml`\n","\n","この計算も，並列実行することが可能です．\n","\n","ここでは，　上記のスクリプト `cv_predict_fmri_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tDd5lDkYvWr"},"outputs":[],"source":["from cv_predict_fmri_fastl2lir import featenc_cv_fastl2lir_predict\n","\n","featenc_cv_fastl2lir_predict(\n","    feature_dir_list,\n","    fmri,\n","    encoder_dir,\n","    output_dir=encoded_fmri_dir,\n","    subjects=list(fmri.keys()),\n","    rois=rois,\n","    label_key=label_key,\n","    layers=features,\n","    feature_index_file=feature_index_file,\n","    excluded_labels=excluded_labels,\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    analysis_name=analysis_name\n",")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["下記のディレクトリに推定された fMRI signal が各刺激ごとに出力されます．\n","\n","`<Save encoded singal dir>/<feature>/<subject>/<ROI>/<Fold>/encoded_fmri/`\n","\n","e.g.) `./data/encoded_fmri/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/encoded_fmri/`\n","\n","- `*.mat`: Predicted fmri signal corresponding to each stimulus\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls ./data/encoded_fmri/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/encoded_fmri/"]},{"cell_type":"markdown","metadata":{"id":"5IpiS8BXYvxP"},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．\n","\n","`$ python src/check_training_prediction.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml --override +wash=predict`\n","\n","ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します．下記の2つの結果が出力されることを確認してください．\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","\n","==========\n","All predicted files exist.\n","==========\n","```\n","もし，未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":5,"status":"error","timestamp":1723865579149,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"FVy2vASBYyMQ","outputId":"f9e9f92d-8c12-4fc4-b847-982faab1532c"},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"predict\"\n","analysis_type = \"encoding\"\n","\n","check_training_prediction(\n","    encoded_fmri_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"-mL8v4zVZSmA"},"source":["これで encoder の prediction は完了です．"]},{"cell_type":"markdown","metadata":{"id":"4S2L9SN0ZXOt"},"source":["### Encoding prediction の performance の計算 （~5分）\n","\n","Encoding結果の評価を行います．具体的には，前セクションで推定されたfMRI signalと，実際に観測されたfMRI singalについて，voxelごとにcorrelation coefficientを計算します．\n","カレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください．\n","\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます\n","\n","`$ python src/cv_evaluate_predicted_fmri.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml `\n","\n","この計算は，並列実行することが可能です．デモでは不要ですが，全ての条件を計算する場合，複数台のマシンでの並列実行を推奨します．\n","\n","ここでは，　上記のスクリプト `cv_evaluate_predicted_fmri.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGO8YQypY2Fz"},"outputs":[],"source":["from cv_evaluate_predicted_fmri import cv_evaluate_predicted_fmri\n","\n","cv_evaluate_predicted_fmri(\n","    encoded_fmri_dir,\n","    encoder_dir,\n","    fmri,\n","    rois,\n","    evaluation_rois,\n","    features,\n","    cv_folds,\n","    label_key,\n","    output_file_pooled=os.path.join(encoded_fmri_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(encoded_fmri_dir, 'evaluation_fold.db'),\n","    average_sample=average_sample,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["下記のディレクトリに評価結果を格納した `sqlite3` のデータベースファイルが保存されます．アクセス方法については，次セクションにおけるコード中の`Read accuracy`の記述を参考にしてください．\n","\n","`<Save encoded singal dir>/evaluation.db`\n","\n","e.g.) `./data/encoded_fmri/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/evaluation.db`\n"]},{"cell_type":"markdown","metadata":{"id":"Q6prb0qDZyjG"},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．\n","\n","`$ python src/check_evaluate_database.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml `\n","\n","ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します． `All conditions are finished` が二度出力されていることを確認してください．一つは各foldの計算が完了していることを示しており，もう一つはfoldの計算結果を平均したpooledの計算が完了していることを示しています．\n","```\n","==========\n","All conditions are finished.\n","==========\n","```\n","もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIoxFmPNZ1hh"},"outputs":[],"source":["from check_evaluate_database import check_evaluate_database\n","\n","analysis_type = \"encoding\"\n","\n","check_evaluate_database(\n","    encoded_fmri_dir,\n","    analysis_type,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n","    output_file_pooled=os.path.join(encoded_fmri_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(encoded_fmri_dir, 'evaluation_fold.db'),\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"VS9mLkKuZ2bW"},"source":["これで encoding analysis の全てのステップが完了しました．最後に可視化を行います．"]},{"cell_type":"markdown","metadata":{"id":"INgiJvxAYywd"},"source":["### Encoding performance の可視化 （~5分）\n","\n","ここでは前セクションで計算されたcorrelation coefficient，すなわちencoding accuracyを可視化します．\n","まずは，category と dimension score による voxel ごとの encoding accuracy の対応関係をplotします．これは本稿の Fig.4B の簡易版に該当します．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig.4B Prediction accuracies of individual voxels\n","\n","from scipy.stats import gaussian_kde\n","import random\n","\n","plt.rcParams[\"font.size\"] = 20\n","linewidth = 1\n","colormap = \"summer\"\n","plotsize = 20\n","metric = \"profile_correlation\"\n","\n","# Sub function\n","def get_density_plot(ax, x, y, limit_datasize = 10000, regression_line = True, diagonal_lines = True, \n","                     linewidth=1, plotsize=3, colormap=\"jet\"):\n","    # nan値の除去（これは必ず行うこと）\n","    not_nans = np.logical_not(np.logical_or(np.isnan(x), np.isnan(y)))\n","    x = x[not_nans]\n","    y = y[not_nans]    \n","    # data数を制約する（defaultでは最大10000）\n","    if limit_datasize is not None:\n","        if len(x) > limit_datasize:\n","            selector = random.choices(np.arange(len(x)), k = limit_datasize)\n","            x = x[selector]\n","            y = y[selector]\n","    # 回帰直線\n","    if regression_line:\n","        p = np.polyfit(x, y, 1)\n","        ax.axline((0, p[1]),  slope=p[0], color=\"r\", lw = linewidth)\n","    # 対角線\n","    if diagonal_lines:\n","        ax.axline((0, 0), slope=1, color=\"black\", lw = linewidth)\n","        ax.axvline(0, c=\"k\", lw=linewidth)\n","        ax.axhline(0, c=\"k\", lw=linewidth)    \n","    # KDE probability\n","    xy = np.vstack([x, y]) \n","    z = gaussian_kde(xy)(xy)\n","    # zの値で並び替え→x,yも並び替えての描画\n","    idx = z.argsort() \n","    x, y, z = x[idx], y[idx], z[idx]\n","    im = ax.scatter(x, y, c=z, s=plotsize, cmap=colormap)    \n","    \n","    return ax, im, p[0]\n","\n","# Load accuracy\n","evaluation_filepath = os.path.join(encoded_fmri_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","for subject in fmri.keys():        \n","    # Load accuracy\n","    category_acc = []\n","    dimension_acc = []\n","    for roi, evaluation_roi in product(rois.keys(), evaluation_rois.keys()):\n","        category_acc.append(db.get(subject=subject, roi=evaluation_roi, layer='category', metric=metric))\n","        dimension_acc.append(db.get(subject=subject, roi=evaluation_roi, layer='dimension', metric=metric))\n","    category_acc = np.hstack(category_acc)     \n","    dimension_acc = np.hstack(dimension_acc)\n","    # Draw figure\n","    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n","    ax.axvline(0.111, c=\"gray\", ls=\"dashed\", lw = linewidth)\n","    ax.axhline(0.111, c=\"gray\", ls=\"dashed\", lw = linewidth)\n","    _, _, slope = get_density_plot(ax, category_acc, dimension_acc,\n","                                   limit_datasize = 10000, regression_line=True, diagonal_lines = True, \n","                                   linewidth =linewidth, plotsize=plotsize, colormap=colormap)\n","    angle = np.arctan(slope)*(180/np.pi)\n","    ax.text(0.3, -0.05, \"Subject1\")\n","    ax.text(-0.07, 0.5, \"Slope angle = %.1f°\" % (angle))\n","    ax.set_xlabel(\"Prediction accuracy(r)\\n[category]\")\n","    ax.set_ylabel(\"Prediction accuracy(r)\\n[dimension]\")\n","    ax.set_xlim(-0.15, 0.55)\n","    ax.set_ylim(-0.15, 0.55)\n","    ax.set_aspect(\"equal\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VFmWEew6pB1d"},"source":["次に `pycortex`を使用してcorrelation coefficientを皮質上にマップします．\n","\n","まずは下記の2点を確認してください．\n","\n","1. `pycortex` の `filestore` が `./data/pycortex` である\n","2. `INKSCAPE_VERSION`が`None`ではないことを確認してください．\n","\n","条件が満たされていない場合，Setup sectionを確認してください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgCOcw5MpGKn"},"outputs":[],"source":["import cortex\n","print('pycortex db:', cortex.config['basic']['filestore'])\n","print('INKSCAPE_VERSION:', cortex.testing_utils.INKSCAPE_VERSION)\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","下記のセルを実行して，結果の可視化を行ってください．各被験者のemotion scoreごとのencoding accuracyが表示されます．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig.4A Prediction accuracies of emotion encoding models\n","\n","pycortex_align = 'voxel2mm_fmriprep'\n","pycortex_overlay_filepath = './data/pycortex/{}/overlays.svg'\n","metric = \"profile_correlation\"\n","vmin = 0.1\n","vmax = 0.4\n","\n","# Load accuracy\n","evaluation_filepath = os.path.join(encoded_fmri_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","for subject in fmri.keys():\n","    # Load Bdata\n","    bdata = bdpy.BData(fmri[subject][0])\n","            \n","    for feature in features:\n","        print(\"Feature:\", feature)\n","        # Create emty volume\n","        # Colormap (cmap) for visualization is set here. Available cmap is same as Matplotlib \n","        # (See document for matplotlib.pylab.colormaps()).\n","        voxel_vol = cortex.Volume.empty(subject, \n","                                        pycortex_align, \n","                                        value=np.nan, cmap='hot')                \n","        \n","        for roi, evaluation_roi in product(rois.keys(), evaluation_rois.keys()):\n","            # Load voxel information\n","            _, encoding_roi_selector = bdata.select(rois[roi], return_index = True)\n","            _, visualize_roi_selector = bdata.select(evaluation_rois[evaluation_roi], return_index = True)\n","            voxel_selector = visualize_roi_selector[encoding_roi_selector]\n","\n","            # Apply voxel selector to voxel_i/j/k array\n","            voxel_ijk = np.vstack([\n","                bdata.get_metadata('voxel_i')[visualize_roi_selector].astype('int32'),\n","                bdata.get_metadata('voxel_j')[visualize_roi_selector].astype('int32'),\n","                bdata.get_metadata('voxel_k')[visualize_roi_selector].astype('int32'),\n","            ]).astype(int)\n","\n","            # Read accuracy \n","            acc = db.get(subject=subject, roi=evaluation_roi, layer=feature, metric=metric).copy()\n","            \n","            # Mapped to each voxel\n","            acc[acc < vmin] = np.nan\n","            voxel_vol.data[ voxel_ijk[2], voxel_ijk[1], voxel_ijk[0] ] = acc\n","\n","        # Set vmax and vmin for visualization                    \n","        voxel_vol.vmax = vmax\n","        voxel_vol.vmin  = 0\n","\n","        # Set text\n","        text_posi_x = 0.5\n","        text_posi_y = 0.9\n","        show_text = feature\n","        \n","        # Show\n","        fig = cortex.quickshow( \n","                    voxel_vol, with_colorbar=True, with_curvature=True, \n","                    overlay_file=pycortex_overlay_filepath.format(subject),\n","                    labelsize=50, linewidth=3,\n","            )\n","        fig.text(text_posi_x, text_posi_y, show_text, \n","                    horizontalalignment='center', verticalalignment='center', size=50)\n","        plt.show()\n","        "]},{"cell_type":"markdown","metadata":{},"source":["以上でencoding analysisのtutorialは終了です．お疲れ様でした．\n","\n","Encoding のより詳細な計算手続きを知りたい場合は，　[feature-encoding](https://github.com/KamitaniLab/feature-encoding) リポジトリ内の handson/02_encoding.ipynb を参照してください．\n","<br/>\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{},"source":["## Tutorial for decoding analysis\n","\n","このセクションではdecoding analysisの手続きについて説明します．このリポジトリでは，下記の手順でdecodingを実行してきます．基本的な流れはencoding analysisと同一です．\n","1. Decoding 設定ファイルを確認する\n","2. Decoderのtrainingを行う\n","3. Decoding prediction を行う\n","4. Decoding prediction の performance の計算\n","5. Decoding performance の可視化 \n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Decoding 設定ファイルの確認\n","\n","まずは `EmotionVideoNeuralRepresentationPython` の直下にいることを確認してください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#%cd EmotionVideoNeuralRepresentationPython\n","!pwd"]},{"cell_type":"markdown","metadata":{},"source":["次に必要なモジュールをimportしてください．Encodingと同じです．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","from itertools import product\n","from pathlib import Path\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from hydra.experimental import initialize_config_dir, compose\n","import cortex\n","\n","import bdpy\n","from bdpy.dataform import SQLite3KeyValueStore\n","\n","sys.path.insert(0, \"./src\")\n","sys.path.insert(0, \"./feature-encoding\")\n","sys.path.insert(0, \"./feature-decoding\")"]},{"cell_type":"markdown","metadata":{},"source":["それではこのチュートリアルにおけるdecoding anlaysisの計算対象の確認を行います．\n","\n","本稿では， decoding anlaysis として， 各被験者の皮質およびsubcortex脳活動から，emotional category (34 category) および affective dimension (14 dimension) の score の推定を行なっています．\n","\n","実際には5人の被験者に対して，それぞれ6-fold crossvalidationの計算を行っています．しかし，すべての計算を行うと時間がかかるため，ここでは省略のため，下記の条件に絞り込みます．\n","\n","1. 5subjectのうち，1subjectの計算のみを行う\n","2. HCP180([Glasser et al., 2016](https://www-nature-com.kyoto-u.idm.oclc.org/articles/nature18933), 360 ROIs) の各ROIではなく，grouping ROI (22 ROIs) を用いる\n","2. Category score の計算のみを行う\n","2. 6-fold計算のうち，1-foldの計算のみを行う\n","3. 交差検証のfold計算中に，さらに入れ子のfold計算を行い，パラメータの最適化を行うフェーズが存在しない （これは現在のPythonコードリポジトリでは実装されていない機能です． オリジナルの[Matlabコードのリポジトリ](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentation/tree/master?tab=readme-ov-file)をご参照ください．）\n","\n","それでは，今回使用する tutorial 用の config ファイル `./config/tutorial_decoding_emotion_score_cv_paper2020.yaml` を確認しましょう．\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load config data\n","config_file = \"./config/tutorial_decoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg = compose(config_name=config_name)    \n","\n","# Convert settings to variables\n","analysis_name = 'cv_train_decoder_fastl2lir-tutorial_decoding_emotion_score_cv_paper2020'\n","fmri = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg[\"decoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","num_voxel = {\n","    roi[\"name\"]: roi[\"num\"]\n","    for roi in cfg[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","label_key = cfg[\"decoder\"][\"fmri\"][\"label_key\"]\n","feature_dir_list = cfg[\"decoder\"][\"features\"][\"paths\"]\n","features = cfg[\"decoder\"][\"features\"][\"layers\"]\n","feature_index_file = cfg.decoder.features.get(\"index_file\", None)\n","decoder_dir = cfg[\"decoder\"][\"path\"]\n","decoded_feature_dir = cfg[\"decoded_feature\"][\"path\"]\n","cv_folds = cfg.cv.get(\"folds\", None)\n","cv_exclusive = cfg.cv.get(\"exclusive_key\", None)\n","cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]\n","excluded_labels = cfg.decoded_feature.fmri.get(\"exclude_labels\", [])\n","average_sample = cfg.decoded_feature.parameters.get(\"average_sample\", True)\n","\n","# Show settings\n","print(\"====== Decoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri.keys())))\n","print(\"ROIs:     {}\".format(list(rois.keys())))\n","print(\"Features: {}\".format(features))\n","print(\"Folds:    {}\".format(cv_labels))\n","print(\"\")\n","print(\"fMRI data dir:            {}\".format(os.path.split(list(fmri.values())[0][0])[0]))\n","print(\"Feature data dir:         {}\".format(feature_dir_list[0]))\n","print(\"\")\n","print(\"Save decoder dir:         {}\".format(decoder_dir))\n","print(\"Save decoded feature dir: {}\".format(decoded_feature_dir))\n","print(\"===============================\")\n"]},{"cell_type":"markdown","metadata":{},"source":["上記の条件で計算を行っていきます．使用している grouping ROI は Glasser et al., 2016 の定義に従います．左右の半球を分けていません．\n","\n","実際の解析で用いられている設定（`./config/encoding_emotion_score_cv_paper2020.yaml`）についても，下記のセルを実行すると確認できます．すべての条件を計算する場合は，計算量の増加により計算時間がスケールすることに注意してください．（ある程度のスペックのあるローカルマシン環境で計算することを強く推奨します．）"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load config data\n","config_file = \"./config/decoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg2 = compose(config_name=config_name)    \n","\n","# Convert settings to variables\n","fmri2 = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg2[\"decoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois2 = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg2[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","feature_dir_list2 = cfg2[\"decoder\"][\"features\"][\"paths\"]\n","features2 = cfg2[\"decoder\"][\"features\"][\"layers\"]\n","decoder_dir2 = cfg2[\"decoder\"][\"path\"]\n","decoded_feature_dir2 = cfg2[\"decoded_feature\"][\"path\"]\n","cv_folds2 = cfg2.cv.get(\"folds\", None)\n","cv_labels2 = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds2))]\n","\n","# Show settings\n","print(\"====== Decoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri2.keys())))\n","print(\"ROIs:     {}\".format(list(rois2.keys())))\n","print(\"Features: {}\".format(features2))\n","print(\"Folds:    {}\".format(cv_labels2))\n","print(\"\")\n","print(\"fMRI data dir:            {}\".format(os.path.split(list(fmri2.values())[0][0])[0]))\n","print(\"Feature data dir:         {}\".format(feature_dir_list2[0]))\n","print(\"\")\n","print(\"Save decoder dir:         {}\".format(decoder_dir2))\n","print(\"Save decoded feature dir: {}\".format(decoded_feature_dir2))\n","print(\"===============================\")\n"]},{"cell_type":"markdown","metadata":{},"source":["それでは，上記の tutorial のセッティングで計算を行っていきます．"]},{"cell_type":"markdown","metadata":{},"source":["### Decoder training を行う (~5分)\n","Decoderのtrainingを行います． カレントディレクトリが， `EmotionVideoNeuralRepresentationPython` の直下であることを確認してください．（`!pwd`）\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます\n","\n","`$ python feature-decoding/cv_train_decoder_fastl2lir.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml`\n","\n","この計算は，異なるマシン上で並行して実行することが可能です（カレントディレクトリ，および保存先ディレクトリをマシン間で共有している必要があります）．\n","特に，全ての条件を計算する場合，複数マシンでの並列実行を推奨します．\n","\n","ここでは，　上記のスクリプト `cv_train_decoder_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from cv_train_decoder_fastl2lir import featdec_cv_fastl2lir_train\n","\n","print(fmri)\n","\n","featdec_cv_fastl2lir_train(\n","    fmri,\n","    feature_dir_list,\n","    output_dir=decoder_dir,\n","    rois=rois,\n","    num_voxel=num_voxel,\n","    label_key=label_key,\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    layers=features,\n","    feature_index_file=feature_index_file,\n","    alpha=cfg[\"decoder\"][\"parameters\"][\"alpha\"],\n","    chunk_axis=cfg[\"decoder\"][\"parameters\"][\"chunk_axis\"],\n","    analysis_name=analysis_name\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["下記のディレクトリに訓練済み decoder model が出力されます．\n","\n","`<Save decoder dir>/<feature>/<subject>/<ROI>/<Fold>/model/`\n","\n","e.g.) `./data/feature_decoders/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/model/`\n","\n","- `W.mat`: Weight of encoder model\n","- `b.mat`: Bias of encoder model\n","- `x_mean.mat`, `x_norm.mat`: Voxel normalization parameters. \n","- `y_mean.mat`, `y_norm.mat`: Feature normalization parameters."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls ./data/feature_decoders/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/model\n"]},{"cell_type":"markdown","metadata":{},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．実行途中でエラーが発生したり，並列計算したりした場合は，必ず実行してください．\n","\n","`$ python src/check_training_prediction.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml --override +wash=train`\n","\n","ここでは，　上記のスクリプト `` で呼び出されるメイン関数を直接呼び出し，実行します．\n","下記が出力されれば問題ありません．\n","\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","```\n","\n","もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"train\"\n","analysis_type = \"decoding\"\n","\n","check_training_prediction(\n","    decoder_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["これで encoder の training は完了です．"]},{"cell_type":"markdown","metadata":{},"source":["### Decoding prediction を行う（~5分）\n","前セクションでtrainingしたdecoder modelを用いて，predictionを行います．カレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください．\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の prediction が開始されます\n","\n","`$ python feature-decoding/cv_predict_feature_fastl2lir.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml`\n","\n","この計算も，並列実行することが可能です．\n","\n","ここでは，　上記のスクリプト `cv_predict_feature_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from cv_predict_feature_fastl2lir import featdec_cv_fastl2lir_predict\n","\n","featdec_cv_fastl2lir_predict(\n","    fmri,\n","    decoder_dir,\n","    output_dir=decoded_feature_dir,\n","    rois=rois,\n","    label_key=label_key,\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    layers=features,\n","    feature_index_file=feature_index_file,\n","    excluded_labels=excluded_labels,\n","    average_sample=average_sample,\n","    chunk_axis=cfg[\"decoder\"][\"parameters\"][\"chunk_axis\"],\n","    analysis_name=analysis_name\n",")"]},{"cell_type":"markdown","metadata":{},"source":["下記のディレクトリに推定された emotional scores が各刺激ごとに出力されます．\n","\n","`<Save decoded feature dir>/<feature>/<subject>/<ROI>/<Fold>/decoded_features/`\n","\n","e.g.) `./data/decoded_features/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/decoded_features/`\n","\n","- `*.mat`: Predicted scores corresponding to each stimulus\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls ./data/decoded_features/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/decoded_features/"]},{"cell_type":"markdown","metadata":{},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．\n","\n","`$ python src/check_training_prediction.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml --override +wash=predict`\n","\n","ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します．下記の2つの結果が出力されることを確認してください．\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","\n","==========\n","All predicted files exist.\n","==========\n","```\n","もし，未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"predict\"\n","analysis_type = \"decoding\"\n","\n","check_training_prediction(\n","    decoded_feature_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["これで decoder の prediction は完了です．"]},{"cell_type":"markdown","metadata":{},"source":["### Decoding prediction の performance の計算 （~5分）\n","\n","Decoding結果の評価を行います．具体的には，前セクションで推定されたemotional scoreと，クラウドソーシングで得られた平均scoreについて，categoryごとにcorrelation coefficientを計算します．\n","カレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください．\n","\n","\n","もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます\n","\n","`$ python src/cv_evaluate_predicted_features.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml `\n","\n","この計算は，並列実行することが可能です．デモでは不要ですが，全ての条件を計算する場合，複数台のマシンでの並列実行を推奨します．\n","\n","ここでは，　上記のスクリプト `cv_evaluate_predicted_features.py` で呼び出されるメイン関数を直接呼び出し，実行します．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from cv_evaluate_predicted_features import cv_evaluate_predicted_features\n","\n","cv_evaluate_predicted_features(\n","    decoded_feature_dir,\n","    feature_dir_list[0],\n","    output_file_pooled=os.path.join(decoded_feature_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(decoded_feature_dir, 'evaluation_fold.db'),\n","    subjects=fmri,\n","    rois=rois,\n","    layers=features,\n","    cv_folds=cv_folds,\n","    feature_index_file=feature_index_file,\n","    feature_decoder_path=decoder_dir,\n","    average_sample=average_sample,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["下記のディレクトリに評価結果を格納した `sqlite3` のデータベースファイルが保存されます．アクセス方法については，次セクションにおけるコード中の`Read accuracy`の記述を参考にしてください．\n","\n","`<Save decoded feature dir>/evaluation.db`\n","\n","e.g.) `./data/decoded_features/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/evaluation.db`\n"]},{"cell_type":"markdown","metadata":{},"source":["計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．\n","\n","`$ python src/check_evaluate_database.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml `\n","\n","ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します． `All conditions are finished` が二度出力されていることを確認してください．一つは各foldの計算が完了していることを示しており，もう一つはfoldの計算結果を平均したpooledの計算が完了していることを示しています．\n","```\n","==========\n","All conditions are finished.\n","==========\n","```\n","もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from check_evaluate_database import check_evaluate_database\n","\n","analysis_type = \"decoding\"\n","\n","check_evaluate_database(\n","    decoded_feature_dir,\n","    analysis_type,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n","    output_file_pooled=os.path.join(decoded_feature_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(decoded_feature_dir, 'evaluation_fold.db'),\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["これで decoding analysis の全てのステップが完了しました．最後に可視化を行います．"]},{"cell_type":"markdown","metadata":{},"source":["### Decoding accuracy の可視化 （~5分）\n","\n","ここでは前セクションで計算されたcorrelation coefficient，すなわちdecoding accuracyを可視化します．\n","まずは，各カテゴリ，各ROI の accuracy を swarmplot で描画します．これは本稿の Fig.2A の簡易版に該当します．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig.2A\n","import pandas as pd\n","import seaborn as sns\n","\n","category_index_list = {\n","    30: 'sexual_desire', \n","    2: 'aesthetic_appreciation', \n","    16: 'entrancement', \n","    14: 'disgust', \n","    3: 'amusement', \n","    19: 'fear', \n","    5: 'anxiety', \n","    22: 'interest', \n","    31: 'surprise', \n","    23: 'joy', \n","    21: 'horror', \n","    1: 'adoration', \n","    9: 'calmness', \n","    27: 'romance', \n","    6: 'awe', \n","    24: 'nostalgia', \n","    12: 'craving', \n","    15: 'empathic_pain', \n","    26: 'relief', \n","    7: 'awkwardness', \n","    18: 'excitement', \n","    28: 'sadness', \n","    8: 'boredom', \n","    33: 'triumph',    \n","    0: 'admiration',\n","    29: 'satisfaction', \n","    32: 'sympathy', \n","    4: 'anger', \n","    10: 'confusion', \n","    13: 'disappointment', \n","    25: 'pride', \n","    17: 'envy', \n","    11: 'contempt', \n","    20: 'guilt'\n","}\n","\n","# Load accuracy\n","evaluation_filepath = os.path.join(decoded_feature_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","dataframe = {\"subject\": [], \"roi\": [], \"category\": [], \"accuracy\": []}\n","for subject, roi, (cat_i, category) in product(fmri.keys(), rois.keys(), category_index_list.items()):                \n","    # Read accuracy \n","    acc = db.get(subject=subject, roi=roi, layer=feature, metric=metric)[cat_i]\n","    dataframe[\"subject\"].append(subject)\n","    dataframe[\"roi\"].append(roi)\n","    dataframe[\"category\"].append(category)\n","    dataframe[\"accuracy\"].append(acc)\n","    \n","dataframe = pd.DataFrame.from_dict(dataframe)\n","display(dataframe)\n","\n","fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n","ax.axhline(0, c=\"k\")\n","for i in [0.2, 0.4, 0.6]:\n","    ax.axhline(i, c=\"lightgray\")\n","sns.swarmplot(data=dataframe, x=\"category\", y=\"accuracy\", ax=ax)\n","ax.set_xticklabels(ax.get_xticklabels(),rotation = 45, ha=\"right\")\n","ax.text(29, 0.4, \"1 subject\\nn=22\", fontsize=20)\n","ax.set_title(\"Emotion category\", fontsize=20)\n","ax.spines['right'].set_visible(False)\n","ax.spines['top'].set_visible(False)\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["次に，`pycortex`を使用してcorrelation coefficientを皮質上にマップします．これは本稿の Fig.2D の簡易版に該当します．\n","\n","まず下記の2点を確認してください．※ この作業をencoding analysisですでに行なっている場合はskipしてください．\n","\n","1. `pycortex` の `filestore` が `./data/pycortex` である\n","2. `INKSCAPE_VERSION`が`None`ではないことを確認してください．\n","\n","条件が満たされていない場合，Setup sectionを確認してください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cortex\n","print('pycortex db:', cortex.config['basic']['filestore'])\n","print('INKSCAPE_VERSION:', cortex.testing_utils.INKSCAPE_VERSION)\n"]},{"cell_type":"markdown","metadata":{},"source":["下記のセルを実行して，結果の可視化を行ってください．各被験者のROIごとのdecoding accuracyが表示されます．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig.4A Cortical surface maps of decoding accuracies for individual categories\n","\n","pycortex_align = 'voxel2mm_fmriprep'\n","pycortex_overlay_filepath = './data/pycortex/{}/overlays.svg'\n","metric = \"profile_correlation\"\n","vmin = 0.0\n","vmax = 100.0\n","feature = \"category\"\n","category_index_list = {\n","    30: 'sexual_desire', \n","    2: 'aesthetic_appreciation', \n","    16: 'entrancement', \n","}\n","\n","# Load accuracy\n","evaluation_filepath = os.path.join(decoded_feature_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","for subject in fmri.keys():\n","    # Load Bdata\n","    bdata = bdpy.BData(fmri[subject][0])\n","                \n","    for cat_i, category in category_index_list.items():\n","        print(\"Category:\", category)\n","        \n","        # Create emty volume\n","        # Colormap (cmap) for visualization is set here. Available cmap is same as Matplotlib \n","        # (See document for matplotlib.pylab.colormaps()).\n","        voxel_vol = cortex.Volume.empty(subject, \n","                                        pycortex_align, \n","                                        value=np.nan,\n","                                        cmap='RdGy_r')\n","\n","        # Get category accuracy & min-max normalization\n","        cat_acc = []\n","        for roi in rois.keys():\n","            # Read accuracy \n","            acc = db.get(subject=subject, roi=roi, layer=feature, metric=metric) \n","            cat_acc.append(acc[cat_i])\n","        cat_acc = np.array(cat_acc)\n","        \n","        # Min-max normalize\n","        cat_acc = (cat_acc - np.nanmin(cat_acc)) / (np.nanmax(cat_acc) - np.nanmin(cat_acc)) * 100\n","        \n","        for roi_i, roi in enumerate(rois.keys()):\n","            # Load voxel information\n","            _, voxel_selector = bdata.select(rois[roi], return_index = True)\n","\n","            # Apply voxel selector to voxel_i/j/k array\n","            voxel_ijk = np.vstack([\n","                bdata.get_metadata('voxel_i')[voxel_selector].astype('int32'),\n","                bdata.get_metadata('voxel_j')[voxel_selector].astype('int32'),\n","                bdata.get_metadata('voxel_k')[voxel_selector].astype('int32'),\n","            ]).astype(int)\n","\n","            \n","            # Mapped to each voxel\n","            voxel_vol.data[ voxel_ijk[2], voxel_ijk[1], voxel_ijk[0] ] = cat_acc[roi_i]\n","\n","        # Set vmax and vmin for visualization                    \n","        voxel_vol.vmax = vmax\n","        voxel_vol.vmin = vmin\n","\n","        # Set text\n","        text_posi_x = 0.5\n","        text_posi_y = 0.9\n","        show_text = category\n","        \n","        # Show\n","        fig = cortex.quickshow( \n","                    voxel_vol, with_colorbar=True, with_curvature=True, \n","                    overlay_file=pycortex_overlay_filepath.format(subject),\n","                    labelsize=50, linewidth=3,\n","            )\n","        fig.text(text_posi_x, text_posi_y, show_text, \n","                    horizontalalignment='center', verticalalignment='center', size=50)\n","        plt.show()\n","        "]},{"cell_type":"markdown","metadata":{},"source":["以上でencoding analysisのtutorialは終了です．お疲れ様でした．\n","\n","Encoding のより詳細な計算手続きを知りたい場合は，　[feature-decoding](https://github.com/KamitaniLab/feature-decoding) リポジトリ内の handson/02_decoding.ipynb を参照してください．\n","<br/>\n","<br/>\n","<br/>"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
