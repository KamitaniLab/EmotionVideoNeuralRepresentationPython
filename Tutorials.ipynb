{"cells":[{"cell_type":"markdown","metadata":{"id":"I94g5POJKa3l"},"source":["# Tutorial for the neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions"]},{"cell_type":"markdown","metadata":{"id":"KzYa3rI5Ka3n"},"source":["This notebook provides a tutorial for our paper:  [Horikawa, Cowen, Keltner, and Kamitani (2020) The neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions. iScience](https://www.cell.com/iscience/fulltext/S2589-0042\\(20\\)30245-5?rss=yes).\n","\n","This tutorial corresponds to the Python version discussed in this paper. It is designed for setting up the environment and verifying computational procedures, hence the computational conditions have been greatly simplified.\n","<!--このチュートリアルは，本稿の Python コード実装に対応したものです．環境のセットアップや計算手続きの確認を目的としたものであるため，計算条件が大きく簡易化されています．-->\n","\n","- [KamitaniLab/EmotionVideoNeuralRepresentationPython](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentationPython)\n","\n","For the exact reproduction code of Paper (Horikawa et al., 2020), please refer to the Matlab version available on the github repository below. (The Python version is currently being prepared.)\n","<!--Paper (Horikawa et al., 2020) の厳密な再現コードについては，下記の Matlab版 github リポジトリに格納されています．（Python版は準備中です．）-->\n","\n","- [KamitaniLab/EmotionVideoNeuralRepresentation](https://github.com/KamitaniLab/EmotionVideoNeuralRepresentation)\n","\n","Furthermore, if you wish to learn more about the algorithms for decoding analysis and encoding analysis, please refer to the repositories mentioned below.\n","<!--また，decoding analysis および encoding analysis のアルゴリズムの詳細を学びたい場合は，下記のリポジトリを参照してください．-->\n","\n","- [KamitaniLab/feature-decoding](https://github.com/KamitaniLab/feature-decoding)\n","- [KamitaniLab/feature-encoding](https://github.com/KamitaniLab/feature-encoding)\n","\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"urv_H5UfKa3n"},"source":["## Setup\n","\n","This tutorial can also be executed on Google Colab and primarily uses Google Colab commands and magic commands to demonstrate the procedures.\n","<!--このチュートリアルはGoogle colabでも実行でき，基本的にはGoogle colabのコマンドおよびマジックコマンドで手続きが示されています．-->\n","\n","**If running on a local machine, it is not necessary to execute all the notebook cells in this Setup section. Please perform the commands shown on the below cells on each machine.**\n","<!--**ローカルマシン上で実行する場合，このSetup セクションのノートブックセルは全て実行する必要はありません．** セル上に示されているコマンドライン操作を，各マシンのCUI上で実行してください．-->"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare directory structure (~5min)\n","<!--### ディレクトリ構成の準備-->\n","\n","Clone the repository below and move into the directory. If running on a local machine and have already cloned the repository, skip this step.\n","<!--下記のgithubリポジトリをcloneし，ディレクトリを移動してください．ローカルマシンで実行しており，すでにリポジトリをclone済みの人はこの作業をskipしてください-->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1046,"status":"ok","timestamp":1723883056495,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"WXgvITX-QDpC","outputId":"d1f005e3-df51-4557-c318-fc3a68d774fe"},"outputs":[],"source":["!git clone https://github.com/KamitaniLab/EmotionVideoNeuralRepresentationPython.git\n","# or git@github.com:KamitaniLab/EmotionVideoNeuralRepresentationPython.git \n","\n","%cd EmotionVideoNeuralRepresentationPython\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"xXdtDDA8QdOM"},"source":["Under the directory `EmotionVideoNeuralRepresentationPython`, please clone the following two GitHub repositories.\n","<!--ディレクトリ`EmotionVideoNeuralRepresentationPython`の下に，さらに下記の2つの github リポジトリを clone してください．-->\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIvUmxA6QV7r"},"outputs":[],"source":["!git clone https://github.com/KamitaniLab/feature-decoding.git\n","# or git@github.com:KamitaniLab/feature-decoding.git\n","!git clone https://github.com/KamitaniLab/feature-encoding.git\n","# or git@github.com:KamitaniLab/feature-encoding.git\n"]},{"cell_type":"markdown","metadata":{"id":"Z4kYQdF2Qk1D"},"source":["Please confirm that the directory structure is as follows:\n","<!--ディレクトリ構成が下記のようになっていることを確認してください．-->\n","\n","```\n","EmotionVideoNeuralRepresentationPython (current directory)\n","├── config/\n","├── data/\n","├── env.yaml\n","├── feature-decoding/\n","├── feature-encoding/\n","├── README.md\n","├── requirements.txt\n","├── src/\n","└── Tutorials.ipynb\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwdboIroRGis"},"outputs":[],"source":["!pwd\n","!ls -1"]},{"cell_type":"markdown","metadata":{"id":"WIFNArgdRYAt"},"source":["### Install libraries and update configurations (~10min or more)\n","<!--### 必要なライブラリのインストールと設定の修正-->\n","\n","Use the environment configuration file `requirements.txt` in `EmotionVideoNeuralRepresentationPython` to set up the environment.\n","<!--`EmotionVideoNeuralRepresentationPython` 内の環境設定ファイルを用いて，環境の構築を行います．-->\n","\n","If running on a local machine, instead of executing the following cell, it is recommended to create a conda environment by executing the command below. **Please note that when creating an environment with conda, it may take several tens of minutes to resolve package dependencies.**\n","After setting up the environment, please switch to this `emotional_video_analysis` environment.\n","<!--ローカルマシンで実行している場合は，下記のセルを実行する代わりに， 下記のコマンドを実行し， conda 環境を作成することを推奨します． **condaで環境を作成する場合，パッケージの依存関係の解決に数十分かかる可能性があるのでご注意ください．** -->\n","<!--環境構築後，この `emotional_video_analysis` の環境に移行してください．-->\n","```\n","$ conda env create -f env.yaml\n","$ conda activate emotional_video_analysis \n","# Please switch the notebook conda environment.\n","```\n","\n","\n","(August 19, 2024) If the Python version is 3.11 or higher, some libraries will behave inconsistently. If the Python version on Google Colab is 3.12 or higher, please downgrade the version.\n","<!--（2024/8/19） Pythonのバージョンが3.11以上の場合，一部のライブラリの挙動に不整合が発生します．もし，Google colab上におけるPythonのバージョンが3.11以上の場合，バージョンを下げる措置をとってください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41215,"status":"ok","timestamp":1723886417450,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"pcpmug-ORbnw","outputId":"1fadc147-775f-4598-9ee3-4af3b4652737"},"outputs":[],"source":["!pip install -r ./requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":["If you want to visualize the cortical map using [pycortex](https://github.com/gallantlab/pycortex), please install `pycortex` and [Inkscape](https://wiki.inkscape.org/wiki/Installing_Inkscape).\n","<!--[pycortex](https://github.com/gallantlab/pycortex)を使用した皮質マップの可視化を行う場合，`pycortex`と[Inkscape](https://wiki.inkscape.org/wiki/Installing_Inkscape)をインストールしてください．-->\n","\n","If using Google Colab, please run the following cell. If running on a local machine, please install it according to your environment.\n","<!--Google colabの場合は下記のセルを実行してください．ローカルマシンで実行している場合は使用している環境に合わせて適宜インストールしてください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!sudo apt-get install -y inkscape\n","!pip install pycortex\n"]},{"cell_type":"markdown","metadata":{"id":"XJn0Qz5gnoiV"},"source":["If you are performing visualizations with `pycortex`, you will need to modify the configuration file. Please make the same adjustments if you are running this tutorial on a local machine.\n","<!--`pycortex` での可視化を行う場合は， 設定ファイルを書き換えます．このチュートリアルをローカルマシンで実行している場合も，同様に書き換えてください．-->\n","\n","First, import the `cortex` package.\n","<!--まず一度`cortex`パッケージのimportを行います．-->\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cortex"]},{"cell_type":"markdown","metadata":{},"source":["This operation will generate `~/.config/pycortex/options.cfg`. Modify the `filestore` path in the `[base]` section of this configuration file to be under `data/pycortex`.\n","<!--これにより `~/.config/pycortex/options.cfg` が生成されます．この設定ファイルの `[base]` セクションの `filestore` のパスを `data/pycortex` 以下に修正します．-->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1458,"status":"ok","timestamp":1723869493343,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"LzqzkxbrnwDe","outputId":"b6bd092a-8f04-4c03-ecbb-152e9dc5b43e"},"outputs":[],"source":["!cat  ~/.config/pycortex/options.cfg | grep filestore\n","!cp ~/.config/pycortex/options.cfg ~/.config/pycortex/backup_options.cfg\n","!sed -i 's/filestore = \\/usr\\/share\\/pycortex\\/db/filestore = .\\/data\\/pycortex\\//' ~/.config/pycortex/options.cfg\n","!cat  ~/.config/pycortex/options.cfg | grep filestore\n"]},{"cell_type":"markdown","metadata":{},"source":["After updating, **be sure to restart your runtime (session).** `pycortex` loads the above settings during import.\n","<!--完了後，**必ずランタイム（セッション）を再起動してください．** `pycortex`は import 時に上記の設定の読み込みを行います．-->\n","\n","After restarting, please move back directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--再起動後は再び　`EmotionVideoNeuralRepresentationPython`  の直下に移動してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# If running on a local machine environment, please correct the `cd` destination as appropriate.\n","%cd /content/EmotionVideoNeuralRepresentationPython"]},{"cell_type":"markdown","metadata":{"id":"gINVPAW7Rb_t"},"source":["### Download Data (~5min)\n","<!--### 必要なデータのダウンロード-->\n","\n","Move to the `data` directory and run `download.py`. `download.py` allows you to specify the data to be downloaded via arguments. This time, only the files used for this tutorial will be downloaded:\n","\n","<!--`data` ディレクトリの下に移動し， `download.py` を実行して下さい．`download.py` はダウンロード対象のデータを引数で指定することができます．今回はデモに使用するファイルのみをダウンロードします．-->\n","\n","- demo_fmri: Subject1's fMRI data for tutorial. \n","- demo_pycortex: Subject1's pycortex data\n","- feautres: All emotional scores\n","\n","If you wish to download data other than that for the tutorial, please refer to [data/README.md](data/README.md).\n","<!--チュートリアル用以外のデータをダウンロードする場合は，data/README.mdを参照してください．-->\n","\n","After downloading, exit the `data` directory and move back directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--ダウンロード後は， `data`を抜けて，`EmotionVideoNeuralRepresentationPython`の直下に移動してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ory3PBLzRcMA"},"outputs":[],"source":["%cd data\n","!python download.py demo_fmri\n","!python download.py demo_pycortex\n","!python download.py features\n","%cd ../"]},{"cell_type":"markdown","metadata":{"id":"3_AuaLnMSncK"},"source":["This completes the preparation.\n","<!--これで準備は完了です．-->\n","\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"e2wQvlwjbhEj"},"source":["## Tutorial for encoding analysis\n","\n","In this section, we will explain the procedure for encoding analysis. \n","<!--このセクションではencoding analysisの手続きについて説明します．-->\n","\n","We perform encoding using the following procedure. Furthermore, the decoding analysis described in the next section can be performed using almost the same procedure.\n","<!--下記の手順でencodingを実行します．さらに次のセクションで説明するdecoding analysisについてもほぼ同じ手順で実行できます．-->\n","\n","1. Check the encoding configuration files\n","2. Train the encoder models\n","3. Predict fMRI signals by using the trained encoder model\n","4. Calculate the accuracy of prediction\n","5. Visualize encoding accuracy\n","<!--1. Encoding 設定ファイルを確認する\n","2. Encoderのtrainingを行う\n","3. Encoding prediction を行う\n","4. Encoding prediction の performance の計算\n","5. Encoding performance の可視化 -->\n"]},{"cell_type":"markdown","metadata":{"id":"pcQ29wf2SwDG"},"source":["### Check the encoding configuration files (~5min)\n","<!-- ### Encoding 設定ファイルの確認-->\n","\n","First, make sure you are directly under `EmotionVideoNeuralRepresentationPython`.\n","<!-- まずは `EmotionVideoNeuralRepresentationPython` の直下にいることを確認してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# If running on a local machine environment, please correct the `cd` destination as appropriate.\n","%cd /content/EmotionVideoNeuralRepresentationPython\n","!pwd"]},{"cell_type":"markdown","metadata":{},"source":["Next, import the necessary modules.\n","<!-- 次に必要なモジュールをimportしてください． -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"elapsed":462,"status":"error","timestamp":1723886828688,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"IJjLJeI2qRI8","outputId":"3285249c-33bf-4d2b-ca87-a17e04365f03"},"outputs":[],"source":["import os\n","import sys\n","from itertools import product\n","from pathlib import Path\n","import warnings\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from hydra.experimental import initialize_config_dir, compose\n","import cortex\n","\n","import bdpy\n","from bdpy.dataform import SQLite3KeyValueStore\n","\n","sys.path.insert(0, \"./src\")\n","sys.path.insert(0, \"./feature-encoding\")\n","sys.path.insert(0, \"./feature-decoding\")\n","\n","# Ignore UserWarning\n","warnings.simplefilter('ignore', UserWarning)"]},{"cell_type":"markdown","metadata":{"id":"nQyM2Cc4qSoA"},"source":["Let's confirm the calculation target of encoding analysis in this tutorial.\n","<!--それではこのtutorialにおけるencoding analysisの計算対象の確認を行います．-->\n","\n","In the encoding analysis in our paper, the brain activity of each subject is estimated using the scores of emotional categories (34 categories), affective dimensions (14 dimensions), semantic features (73 feat), and vision features (1000 feat).\n","<!--本稿では， encoding anlaysis として， emotional category (34 category)，affective dimension (14 dimension)，semantic feature (73 feat), vision feature (1000 feat) の score を用いて，各被験者の脳活動を推定しています．-->\n","\n","In reality, 6-fold cross-validation calculations were performed for each of the five subjects. However, since performing all calculations would take time, in this tutorial, the calculations are limited to the following conditions.\n","<!--実際には5人の被験者に対して，それぞれ6-fold crossvalidationの計算を行っています．しかし，すべての計算を行うと時間がかかるため，ここでは下記の条件に絞り込みます．-->\n","\n","1. Only perform calculations for one of the five subjects.\n","2. Only perform calculations for cortical brain activity. (Not perform calculations for subcortex)\n","3. Only perform calculations for category scores and dimension scores.\n","4. Only perform calculations for one of the six-fold calculations.\n","5. Not optimize parameters (do not optimize parameters by performing nested cross-validation during cross-validation)\n","<!--1. 5subjectのうち，1subjectの計算のみを行う\n","2. 皮質脳活動の計算のみを行う（subcortexについて計算しない）\n","3. category score と dimension score の計算のみを行う（semantic and vision score の計算を行わない）\n","4. 6-fold計算のうち，1-foldの計算のみを行う\n","5. 交差検証のfold計算中に，さらにtraining dataset内のfold計算を行ってパラメータの最適化を行うフェーズを実行しない-->\n","\n","Now, let's check the config file for the tutorial we will use this time, `./config/tutorial_encoding_emotion_score_cv_paper2020.yaml`.\n","<!--それでは，今回使用する tutorial 用の config ファイル `./config/tutorial_encoding_emotion_score_cv_paper2020.yaml` を確認しましょう．-->\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":509,"status":"error","timestamp":1723864898176,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"xGH0ATRiTzAJ","outputId":"1910f916-d0db-46db-8c80-4cb8aee0d6c3"},"outputs":[],"source":["# Load config data\n","config_file = \"./config/tutorial_encoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg = compose(config_name=config_name)    \n","\n","# Convert settings to variables\n","analysis_name = 'cv_train_encoder_fastl2lir-tutorial_encoding_emotion_score_cv_paper2020'\n","fmri = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg[\"encoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"encoder\"][\"fmri\"][\"rois\"]\n","}\n","label_key = cfg[\"encoder\"][\"fmri\"][\"label_key\"]\n","feature_dir_list = cfg[\"encoder\"][\"features\"][\"paths\"]\n","features = [feat[\"name\"] for feat in cfg[\"encoder\"][\"features\"][\"layers\"]]\n","num_unit = {\n","    layer[\"name\"]: layer[\"num\"]\n","    for layer in cfg[\"encoder\"][\"features\"][\"layers\"]\n","}\n","feature_index_file = cfg.encoder.features.get(\"index_file\", None)\n","encoder_dir = cfg[\"encoder\"][\"path\"]\n","encoded_fmri_dir = cfg[\"encoded_fmri\"][\"path\"]\n","cv_folds = cfg.cv.get(\"folds\", None)\n","cv_exclusive = cfg.cv.get(\"exclusive_key\", None)\n","cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]\n","excluded_labels = cfg.encoded_fmri.fmri.get(\"exclude_labels\", [])\n","average_sample = cfg.encoded_fmri.parameters.get(\"average_sample\", True)\n","evaluation_rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"evaluation\"][\"fmri\"][\"rois\"]\n","}\n","\n","# Show settings\n","print(\"====== Encoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri.keys())))\n","print(\"ROIs:     {}\".format(list(rois.keys())))\n","print(\"Features: {}\".format(features))\n","print(\"Folds:    {}\".format(cv_labels))\n","print(\"\")\n","print(\"fMRI data dir:           {}\".format(os.path.split(list(fmri.values())[0][0])[0]))\n","print(\"Feature data dir:        {}\".format(feature_dir_list[0]))\n","print(\"\")\n","print(\"Save encoder dir:        {}\".format(encoder_dir))\n","print(\"Save encoded singal dir: {}\".format(encoded_fmri_dir))\n","print(\"===============================\")\n"]},{"cell_type":"markdown","metadata":{},"source":["You can also check the settings used in the analysis on the paper (`./config/encoding_emotion_score_cv_paper2020.yaml`) by executing the cell below.\n","<!--実際の paper の解析で用いられている設定（`./config/encoding_emotion_score_cv_paper2020.yaml`）についても，下記のセルを実行すると確認できます．-->\n","\n","By specifying this config file, you can change the subsequent training and prediction to conform to the analysis of the paper. However, **please note that the parameters will be fixed conditions.** The phase of optimizing parameters by performing further fold calculations in the training dataset during the fold calculation of cross-validation is not implemented in the Python version. \n","<!--こちらに config file の指定を差し替えることで，以降の training や prediction について，paper の解析に準拠したものに変更することができます．**ただし，パラメータが固定された条件になることにご注意ください．**交差検証のfold計算中に，さらにtraining dataset内のfold計算を行ってパラメータの最適化を行うフェーズについては，Python 版で実装されていません． -->\n","In addition, if you calculate all conditions, the required calculation memory will increase, and it may not be possible to run it on Google Colab. \n","Moreover, the calculation time will be long. \n","So please run it in a local machine environment.\n","<!--また，すべての条件を計算する場合は，必要な計算メモリが増大し，Google colabでは実行できない可能性があります．加えて，計算が長時間に及ぶため，ローカルマシン環境で実行するようにしてください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Original encoding analysis settings\n","\n","config_file = \"./config/encoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg2 = compose(config_name=config_name)    \n","\n","# Convert settings\n","fmri2 = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg2[\"encoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois2 = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg2[\"encoder\"][\"fmri\"][\"rois\"]\n","}\n","feature_dir_list2 = cfg2[\"encoder\"][\"features\"][\"paths\"]\n","features2 = [feat[\"name\"] for feat in cfg2[\"encoder\"][\"features\"][\"layers\"]]\n","encoder_dir2 = cfg2[\"encoder\"][\"path\"]\n","encoded_fmri_dir2 = cfg2[\"encoded_fmri\"][\"path\"]\n","cv_folds2 = cfg2.cv.get(\"folds\", None)\n","cv_labels2 = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds2))]\n","\n","from pprint import pprint\n","print(\"====== Encoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri2.keys())))\n","print(\"ROIs:     {}\".format(list(rois2.keys())))\n","print(\"Features: {}\".format(features2))\n","print(\"Folds:    {}\".format(cv_labels2))\n","print(\"\")\n","print(\"fMRI data dir:           {}\".format(os.path.split(list(fmri2.values())[0][0])[0]))\n","print(\"Feature data dir:        {}\".format(feature_dir_list2[0]))\n","print(\"\")\n","print(\"Save encoder dir:        {}\".format(encoder_dir2))\n","print(\"Save encoded singal dir: {}\".format(encoded_fmri_dir2))\n","print(\"===============================\")"]},{"cell_type":"markdown","metadata":{"id":"v41ySd8mZLFZ"},"source":["Now, let's perform the calculation with the settings of the tutorial above.\n","<!--それでは，上記の tutorial のセッティングで計算を行っていきます．-->"]},{"cell_type":"markdown","metadata":{"id":"F8WhQB3qVKEy"},"source":["### Train the encoder models (~5min)\n","\n","In this subsection, we will train the encoder model. Please make sure that the current directory is directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--このサブセクションでは，encoder model のtrainingを行います． カレントディレクトリが， `EmotionVideoNeuralRepresentationPython` の直下であることを確認してください．-->\n","\n","If running on a local machine, you can also perform training the encoder according to the specified config file by executing the command below.\n","<!--もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます-->\n","\n","`$ python feature-encoding/cv_train_encoder_fastl2lir.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml`\n","\n","This calculation can also be run in parallel across multiple machines. (The current directory and the result storage directory must be shared between those machines).\n","Parallel execution on multiple machines can significantly reduce the calculation time.\n","<!--この計算は，異なるマシン上で並行して実行することが可能です（カレントディレクトリ，および保存先ディレクトリをマシン間で共有している必要があります）．\n","特に，全ての条件を計算する場合，複数マシンでの並列実行を推奨します．-->\n","\n","In this tutorial, we will directly call and execute the main function called by the above script `cv_train_encoder_fastl2lir.py`.\n","<!--このチュートリアルでは，　上記のスクリプト `cv_train_encoder_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vT0fJY_V815"},"outputs":[],"source":["from cv_train_encoder_fastl2lir import featenc_cv_fastl2lir_train\n","\n","featenc_cv_fastl2lir_train(\n","    fmri,\n","    feature_dir_list,\n","    output_dir=encoder_dir,\n","    rois=rois,\n","    label_key=label_key,\n","    layers=features,\n","    num_unit=num_unit,\n","    feature_index_file=feature_index_file,\n","    alpha=cfg[\"encoder\"][\"parameters\"][\"alpha\"],\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    analysis_name=analysis_name\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"4iBVOysvUUfF"},"source":["The trained encoder model will be output to the following directory.\n","<!--下記のディレクトリに訓練済みencoderモデルが出力されます．-->\n","\n","`<Save encoder dir>/<feature>/<subject>/<ROI>/<Fold>/model/`\n","\n","e.g.) `./data/feature_encoders/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/model/`\n","\n","- `W.mat`: Weight of encoder model\n","- `b.mat`: Bias of encoder model\n","- `x_mean.mat`, `x_norm.mat`: Feature normalization parameters. \n","- `y_mean.mat`, `y_norm.mat`: Voxel normalization parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1723881101532,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"X8Gs2QDqUQbX","outputId":"0a476a1d-2f89-4e42-efc2-5dbbac6eb596"},"outputs":[],"source":["!ls ./data/feature_encoders/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/model"]},{"cell_type":"markdown","metadata":{"id":"w51vRs0YWNhG"},"source":["After calculating, please run the script below to check that all specified conditions are complete. Be sure to run it if an error occurs during execution or if you are performing parallel calculations.\n","<!--計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．実行途中でエラーが発生したり，並列計算したりした場合は，必ず実行してください．-->\n","\n","`$ python src/check_training_prediction.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml --override +wash=train`\n","\n","Here, directly call and execute the main function called by the above script `check_training_prediction.py`. There is no problem if you get the output below.\n","<!--ここでは，　上記のスクリプト `check_training_prediction.py` で呼び出されるメイン関数を直接呼び出し，実行します．下記が出力されれば問題ありません．-->\n","\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","```\n","\n","If you are informed that there are unfinished conditions, run the above cell (command) again to ensure that all conditions are completed.\n","<!--もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5JsCc_hWYH7"},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"train\"\n","analysis_type = \"encoding\"\n","\n","check_training_prediction(\n","    encoder_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"JSYQPeX9WY34"},"source":["This completes encoder training.\n","<!-- これで encoder の training は完了です． -->\n"]},{"cell_type":"markdown","metadata":{"id":"b-t8chEZYKdW"},"source":["### Predict fMRI signals by using the trained encoder model (~5min)\n","\n","In this subsection, we will perform prediction using the encoder model trained in the previous section. Please make sure that the current directory is directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--このセクションでは，前セクションで訓練されたencoder modelを用いて，predictionを行います．カレントディレクトリが， `EmotionVideoNeuralRepresentationPython` の直下dであることを確認してください．-->\n","\n","If running on a local machine, you can also perform predictions according to the specified config file by executing the command below. This calculation can also be performed in parallel across multiple machines.\n","<!--もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の prediction が開始されます-->\n","<!--この計算も，複数のマシン間で並列実行することが可能です．-->\n","\n","`$ python feature-encoding/cv_predict_fmri_fastl2lir.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml`\n","\n","In this tutorial, we will directly call and execute the main function called by the above script `cv_predict_fmri_fastl2lir.py`.\n","<!--このでは，　上記のスクリプト `cv_predict_fmri_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tDd5lDkYvWr"},"outputs":[],"source":["from cv_predict_fmri_fastl2lir import featenc_cv_fastl2lir_predict\n","\n","featenc_cv_fastl2lir_predict(\n","    feature_dir_list,\n","    fmri,\n","    encoder_dir,\n","    output_dir=encoded_fmri_dir,\n","    subjects=list(fmri.keys()),\n","    rois=rois,\n","    label_key=label_key,\n","    layers=features,\n","    feature_index_file=feature_index_file,\n","    excluded_labels=excluded_labels,\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    analysis_name=analysis_name\n",")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["The predicted fMRI signal for each stimulus will be output to the following directory.\n","<!--下記のディレクトリに推定された fMRI signal が各刺激ごとに出力されます．-->\n","\n","`<Save encoded singal dir>/<feature>/<subject>/<ROI>/<Fold>/encoded_fmri/`\n","\n","e.g.) `./data/encoded_fmri/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/encoded_fmri/`\n","\n","- `*.mat`: Predicted fMRI signal corresponding to each stimulus\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls ./data/encoded_fmri/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/category/Subject1/Cortex/cv-fold1/encoded_fmri/"]},{"cell_type":"markdown","metadata":{"id":"5IpiS8BXYvxP"},"source":["After calculating, please run the script below to check that all specified conditions are complete. Be sure to run it if an error occurs during execution or if you are performing parallel calculations.\n","<!--計算後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認してください．-->\n","\n","`$ python src/check_training_prediction.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml --override +wash=predict`\n","\n","Here, directly call and execute the main function called by the above script `check_training_prediction.py`. There is no problem if you get the following two outputs.\n","\n","<!--ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します．下記の2つの出力が得られれば問題ありません．-->\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","\n","==========\n","All predicted files exist.\n","==========\n","```\n","\n","If you are informed that there are unfinished conditions, run the above cell (command) again to ensure that all conditions are completed.\n","<!--もし，未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．-->\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":5,"status":"error","timestamp":1723865579149,"user":{"displayName":"Misato Tanaka","userId":"16641827196345823828"},"user_tz":-540},"id":"FVy2vASBYyMQ","outputId":"f9e9f92d-8c12-4fc4-b847-982faab1532c"},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"predict\"\n","analysis_type = \"encoding\"\n","\n","check_training_prediction(\n","    encoded_fmri_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"-mL8v4zVZSmA"},"source":["This completes the prediction of the encoding.\n","<!--これで encoding の prediction は完了です．-->"]},{"cell_type":"markdown","metadata":{"id":"4S2L9SN0ZXOt"},"source":["### Calculate the accuracy of prediction (~5min)\n","<!-- ### Encoding prediction の performance の計算 （~5分） -->\n","\n","In this sub section, we will calculate the accuracy of the predicted signal. Specifically, we calculate the correlation coefficient for each voxel between the fMRI signal predicted in the previous section and the actual observed fMRI signal.\n","The current directory should be directly under `EmotionVideoNeuralRepresentationPython`.\n","<!-- このセクションでは，predicted signal の精度を計算します．具体的には，前セクションで推定されたfMRI signalと，実際に観測されたfMRI singalについて，voxelごとにcorrelation coefficientを計算します．\n","カレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください． -->\n","\n","If running on a local machine, you can also evaluate the predicted signal according to the specified config file by executing the command below. This calculation can also be performed in parallel across multiple machines.\n","<!-- もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて推定signalの評価が開始されます\n","-->\n","<!-- この計算は，並列実行することが可能です． -->\n","\n","`$ python src/cv_evaluate_predicted_fmri.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml `\n","\n","In this tutorial, we will directly call and execute the main function called by the above script `cv_evaluate_predicted_fmri.py`.\n","<!-- ここでは，　上記のスクリプト `cv_evaluate_predicted_fmri.py` で呼び出されるメイン関数を直接呼び出し，実行します． -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGO8YQypY2Fz"},"outputs":[],"source":["from cv_evaluate_predicted_fmri import cv_evaluate_predicted_fmri\n","\n","cv_evaluate_predicted_fmri(\n","    encoded_fmri_dir,\n","    encoder_dir,\n","    fmri,\n","    rois,\n","    evaluation_rois,\n","    features,\n","    cv_folds,\n","    label_key,\n","    output_file_pooled=os.path.join(encoded_fmri_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(encoded_fmri_dir, 'evaluation_fold.db'),\n","    average_sample=average_sample,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["The `sqlite3` database file containing the evaluation results will be saved in the following directory. For information on how to access the accuracy values, please refer to the description of `Read accuracy` in the code in the next section.\n","<!-- 下記のディレクトリに評価結果を格納した `sqlite3` のデータベースファイルが保存されます．アクセス方法については，次セクションにおけるコード中の`Read accuracy`の記述を参考にしてください． -->\n","\n","`<Save encoded singal dir>/evaluation.db`\n","\n","e.g.) `./data/encoded_fmri/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/evaluation.db`\n"]},{"cell_type":"markdown","metadata":{"id":"Q6prb0qDZyjG"},"source":["After calculating, please run the script below to check that all specified conditions are complete. Be sure to run it if an error occurs during execution or if you are performing parallel calculations.\n","<!--計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．-->\n","\n","`$ python src/check_evaluate_database.py config/tutorial_encoding_emotion_score_cv_paper2020.yaml `\n","\n","\n","Here, directly call and execute the main function called by the above script `check_evaluate_database.py`. Please confirm that `All conditions are finished` is printed twice. Once indicates that each fold evaluation has been completed, and the other indicates that the pooled evaluation by averaging the fold results, has been completed.\n","<!--ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します． `All conditions are finished` が二度出力されていることを確認してください．一つは各foldの計算が完了していることを示しており，もう一つはfoldの計算結果を平均したpooledの計算が完了していることを示しています．-->\n","```\n","==========\n","All conditions are finished.\n","==========\n","\n","==========\n","All conditions are finished.\n","==========\n","```\n","\n","If you are informed that there are unfinished conditions, run the above cell (command) again to ensure that all conditions are completed.\n","<!-- もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください． -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIoxFmPNZ1hh"},"outputs":[],"source":["from check_evaluate_database import check_evaluate_database\n","\n","analysis_type = \"encoding\"\n","\n","check_evaluate_database(\n","    encoded_fmri_dir,\n","    analysis_type,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n","    output_file_pooled=os.path.join(encoded_fmri_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(encoded_fmri_dir, 'evaluation_fold.db'),\n",")"]},{"cell_type":"markdown","metadata":{"id":"VS9mLkKuZ2bW"},"source":["Now all the calculations for encoding analysis are complete. Finally, we will visualize it.\n","<!-- これで encoding analysis の全てのステップが完了しました．最後に可視化を行います．-->"]},{"cell_type":"markdown","metadata":{"id":"INgiJvxAYywd"},"source":["### Visualize encoding accuracy (~10min)\n","<!--### Encoding performance の可視化 （~10分）-->\n","\n","In this sub section, we visualize the encoding accuracy calculated in the previous section.-->\n","<!--このセクションでは，前セクションで計算されたencoding accuracyを可視化します．-->\n","\n","First, we plot the correspondence between the encoding accuracy of each voxel obtained from the category score and dimension score. This corresponds to a simplified version of Fig. 4B in our paper. Please note that the results does not match exactly because the calculation conditions are reduced.\n","<!--まずは，category score と dimension score のそれぞれから得られた，各voxel の encoding accuracy の対応関係をplotします．これは本稿の Fig.4B の簡易版に該当します．計算条件が緩和されているため，厳密一致しないことには注意してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig.4B Prediction accuracies of individual voxels\n","\n","from scipy.stats import gaussian_kde\n","import random\n","\n","plt.rcParams[\"font.size\"] = 20\n","linewidth = 1\n","colormap = \"summer\"\n","plotsize = 20\n","metric = \"profile_correlation\"\n","\n","# Sub function\n","def get_density_plot(ax, x, y, limit_datasize = 10000, regression_line = True, diagonal_lines = True, \n","                     linewidth=1, plotsize=3, colormap=\"jet\"):\n","    # nan値の除去（これは必ず行うこと）\n","    not_nans = np.logical_not(np.logical_or(np.isnan(x), np.isnan(y)))\n","    x = x[not_nans]\n","    y = y[not_nans]    \n","    # data数を制約する（defaultでは最大10000）\n","    if limit_datasize is not None:\n","        if len(x) > limit_datasize:\n","            selector = random.choices(np.arange(len(x)), k = limit_datasize)\n","            x = x[selector]\n","            y = y[selector]\n","    # 回帰直線\n","    if regression_line:\n","        p = np.polyfit(x, y, 1)\n","        ax.axline((0, p[1]),  slope=p[0], color=\"r\", lw = linewidth)\n","    # 対角線\n","    if diagonal_lines:\n","        ax.axline((0, 0), slope=1, color=\"black\", lw = linewidth)\n","        ax.axvline(0, c=\"k\", lw=linewidth)\n","        ax.axhline(0, c=\"k\", lw=linewidth)    \n","    # KDE probability\n","    xy = np.vstack([x, y]) \n","    z = gaussian_kde(xy)(xy)\n","    # zの値で並び替え→x,yも並び替えての描画\n","    idx = z.argsort() \n","    x, y, z = x[idx], y[idx], z[idx]\n","    im = ax.scatter(x, y, c=z, s=plotsize, cmap=colormap)    \n","    \n","    return ax, im, p[0]\n","\n","# Connect to accuracy database\n","evaluation_filepath = os.path.join(encoded_fmri_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","for subject in fmri.keys():        \n","    # Load accuracy\n","    category_acc = []\n","    dimension_acc = []\n","    for roi, evaluation_roi in product(rois.keys(), evaluation_rois.keys()):\n","        category_acc.append(db.get(subject=subject, roi=evaluation_roi, layer='category', metric=metric))\n","        dimension_acc.append(db.get(subject=subject, roi=evaluation_roi, layer='dimension', metric=metric))\n","    category_acc = np.hstack(category_acc)     \n","    dimension_acc = np.hstack(dimension_acc)\n","    # Draw figure\n","    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n","    ax.axvline(0.111, c=\"gray\", ls=\"dashed\", lw = linewidth)\n","    ax.axhline(0.111, c=\"gray\", ls=\"dashed\", lw = linewidth)\n","    _, _, slope = get_density_plot(ax, category_acc, dimension_acc,\n","                                   limit_datasize = 10000, regression_line=True, diagonal_lines = True, \n","                                   linewidth =linewidth, plotsize=plotsize, colormap=colormap)\n","    angle = np.arctan(slope)*(180/np.pi)\n","    ax.text(0.3, -0.05, \"Subject1\")\n","    ax.text(-0.07, 0.5, \"Slope angle = %.1f°\" % (angle))\n","    ax.set_xlabel(\"Prediction accuracy(r)\\n[category]\")\n","    ax.set_ylabel(\"Prediction accuracy(r)\\n[dimension]\")\n","    ax.set_xlim(-0.15, 0.55)\n","    ax.set_ylim(-0.15, 0.55)\n","    ax.set_aspect(\"equal\")"]},{"cell_type":"markdown","metadata":{"id":"VFmWEew6pB1d"},"source":["Next, we use `pycortex` to map the accuracy of each voxel onto the flattend cortex of the subject.\n","<!--次に `pycortex`を使用して，各voxelのcorrelation coefficientを被験者の展開された皮質上にマップします．-->\n","\n","Before doing this, please check the following two points.\n","<!--まずは下記の2点を確認してください．-->\n","\n","1. The `filestore` of `pycortex` is `./data/pycortex`.\n","2. `INKSCAPE_VERSION` is not `None`.\n","\n","If the conditions are not met, please check the Setup section.\n","<!--条件が満たされていない場合，Setup sectionを確認してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgCOcw5MpGKn"},"outputs":[],"source":["import cortex\n","print('pycortex filestore:', cortex.config['basic']['filestore'])\n","print('INKSCAPE_VERSION:', cortex.testing_utils.INKSCAPE_VERSION)\n"]},{"cell_type":"markdown","metadata":{},"source":["If the above conditions are met, run the cell below to visualize the results.\n","The encoding accuracy for each subject's emotion score will be displayed.\n","<!--上記の条件が満たされているならば，下記のセルを実行して，結果の可視化を行ってください．\n","各被験者のemotion scoreごとのencoding accuracyが表示されます．-->\n","\n","This corresponds to a simplified version of Fig. 4A in our paper. Please note that the results does not match exactly because the calculation conditions are reduced.\n","<!--これは本稿の Fig.4A の簡易版に該当します．計算条件が緩和されているため，厳密一致しないことには注意してください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig.4A Prediction accuracies of emotion encoding models\n","\n","pycortex_align = 'voxel2mm_fmriprep'\n","pycortex_overlay_filepath = './data/pycortex/{}/overlays.svg'\n","metric = \"profile_correlation\"\n","vmin = 0.1\n","vmax = 0.4\n","\n","# Connect to accuracy database\n","evaluation_filepath = os.path.join(encoded_fmri_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","for subject in fmri.keys():\n","    # Load Bdata\n","    bdata = bdpy.BData(fmri[subject][0])\n","            \n","    for feature in features:\n","        print(\"Feature:\", feature)\n","        # Create emty volume\n","        # Colormap (cmap) for visualization is set here. Available cmap is same as Matplotlib \n","        # (See document for matplotlib.pylab.colormaps()).\n","        voxel_vol = cortex.Volume.empty(subject, \n","                                        pycortex_align, \n","                                        value=np.nan, cmap='hot')                \n","        \n","        for roi, evaluation_roi in product(rois.keys(), evaluation_rois.keys()):\n","            # Load voxel information\n","            #_, encoding_roi_selector = bdata.select(rois[roi], return_index = True)\n","            _, visualize_roi_selector = bdata.select(evaluation_rois[evaluation_roi], return_index = True)\n","            #voxel_selector = visualize_roi_selector[encoding_roi_selector]\n","\n","            # Apply voxel selector to voxel_i/j/k array\n","            voxel_ijk = np.vstack([\n","                bdata.get_metadata('voxel_i')[visualize_roi_selector].astype('int32'),\n","                bdata.get_metadata('voxel_j')[visualize_roi_selector].astype('int32'),\n","                bdata.get_metadata('voxel_k')[visualize_roi_selector].astype('int32'),\n","            ]).astype(int)\n","\n","            # Read accuracy \n","            acc = db.get(subject=subject, roi=evaluation_roi, layer=feature, metric=metric).copy()\n","            \n","            # Mapped to each voxel\n","            acc[acc < vmin] = np.nan\n","            voxel_vol.data[ voxel_ijk[2], voxel_ijk[1], voxel_ijk[0] ] = acc\n","\n","        # Set vmax and vmin for visualization                    \n","        voxel_vol.vmax = vmax\n","        voxel_vol.vmin  = 0\n","\n","        # Set text\n","        text_posi_x = 0.5\n","        text_posi_y = 0.9\n","        show_text = feature\n","        \n","        # Show\n","        fig = cortex.quickshow( \n","                    voxel_vol, with_colorbar=True, with_curvature=True, \n","                    overlay_file=pycortex_overlay_filepath.format(subject),\n","                    labelsize=50, linewidth=3,\n","            )\n","        fig.text(text_posi_x, text_posi_y, show_text, \n","                    horizontalalignment='center', verticalalignment='center', size=50)\n","        plt.show()\n","        "]},{"cell_type":"markdown","metadata":{},"source":["This completes the encoding analysis tutorial. Congratulations on your hard work!\n","<!-- 以上でencoding analysisのtutorialは完了です．お疲れ様でした． -->\n","\n","If you would like to know more about the encoding calculation procedure, please refer to [01_encoding.ipynb](https://github.com/KamitaniLab/feature-encoding/blob/main/handson/01_encoding.ipynb) in the [feature-encoding](https://github.com/KamitaniLab/feature-encoding) repository.\n","<!-- Encoding のより詳細な計算手続きを知りたい場合は，　[feature-encoding](https://github.com/KamitaniLab/feature-encoding) リポジトリ内の handson/02_encoding.ipynb を参照してください． -->\n","\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{},"source":["## Tutorial for decoding analysis\n","\n","In this section, we will explain the procedure for decoding analysis. \n","<!--このセクションではdecoding analysisの手続きについて説明します．-->\n","\n","We perform decoding using the following procedure. The basic flow is the same as in the previous encoding section.\n","<!--下記の手順でdecodingを実行します．基本的な流れは前のencodingセクションと同一です．-->\n","\n","1. Check the decoding configuration files\n","2. Train the decoder models\n","3. Predict emotional scores by using the trained decoder models\n","4. Calculate the accuracy of prediction\n","5. Visualize decoding performance\n","<!--1. Decoding 設定ファイルを確認する\n","2. Decoderのtrainingを行う\n","3. Decoding prediction を行う\n","4. Decoding prediction の performance の計算\n","5. Decoding performance の可視化 -->\n"]},{"cell_type":"markdown","metadata":{},"source":["### Check the decoding configuration files (~5min)\n","<!--### Decoding 設定ファイルの確認-->\n","\n","First, make sure you are directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--まずは `EmotionVideoNeuralRepresentationPython` の直下にいることを確認してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# If running on a local machine environment, please correct the `cd` destination as appropriate.\n","%cd /content/EmotionVideoNeuralRepresentationPython\n","!pwd"]},{"cell_type":"markdown","metadata":{},"source":["Next, import the necessary modules.\n","<!--次に必要なモジュールをimportしてください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","from itertools import product\n","from pathlib import Path\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from hydra.experimental import initialize_config_dir, compose\n","import cortex\n","\n","import bdpy\n","from bdpy.dataform import SQLite3KeyValueStore\n","\n","sys.path.insert(0, \"./src\")\n","sys.path.insert(0, \"./feature-encoding\")\n","sys.path.insert(0, \"./feature-decoding\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's confirm the calculation target of decoding analysis in this tutorial.\n","<!--それではこのチュートリアルにおけるdecoding anlaysisの計算対象の確認を行います．-->\n","\n","In the decoding analysis in our paper, we estimate scores for emotional categories (34 categories) and affective dimensions (14 dimensions) from each subject's cortical and subcortex brain activity.\n","<!--本稿では， decoding anlaysis として， 各被験者の皮質およびsubcortex脳活動から，emotional category (34 category) および affective dimension (14 dimension) の score の推定を行なっています．-->\n","\n","In reality, 6-fold cross-validation calculations were performed for each of the five subjects. However, since performing all calculations would take time, in this tutorial, the calculations are limited to the following conditions.\n","<!--実際には5人の被験者に対して，それぞれ6-fold crossvalidationの計算を行っています．しかし，すべての計算を行うと時間がかかるため，ここでは省略のため，下記の条件に絞り込みます．-->\n","\n","1. Only perform calculations for one of the five subjects.\n","2. Use grouping 22 ROIs instead of 360 ROIs of HCP180 ([Glasser et al., 2016](https://www-nature-com.kyoto-u.idm.oclc.org/articles/nature18933)).\n","3. Only perform calculations for category scores. \n","4. Only perform calculations for one of the six-fold calculations.\n","5. Not optimize parameters (do not optimize parameters by performing nested cross-validation during cross-validation)\n","<!--1. 5subjectのうち，1subjectの計算のみを行う\n","2. HCP180([Glasser et al., 2016](https://www-nature-com.kyoto-u.idm.oclc.org/articles/nature18933), 360 ROIs) の各ROIではなく，grouping ROI (22 ROIs) を用いる\n","2. Category score の計算のみを行う\n","2. 6-fold計算のうち，1-foldの計算のみを行う\n","3. 交差検証のfold計算中に，さらにtraining dataset内のfold計算を行ってパラメータの最適化を行うフェーズを実行しない-->\n","\n","The grouping 22 ROIs are defined in the HCP180 paper. The left and right hemispheres are not treated separately.\n","<!--ここで使用する grouping 22 ROIs は HCP180 の paper 中で定義されたものです．左右の半球を分けずに扱います．-->\n","\n","Now, let's check the config file for the tutorial we will use this time, `./config/tutorial_decoding_emotion_score_cv_paper2020.yaml`.\n","<!--それでは，今回使用する tutorial 用の config ファイル `./config/tutorial_decoding_emotion_score_cv_paper2020.yaml` を確認しましょう．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load config data\n","config_file = \"./config/tutorial_decoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg = compose(config_name=config_name)    \n","\n","# Convert settings to variables\n","analysis_name = 'cv_train_decoder_fastl2lir-tutorial_decoding_emotion_score_cv_paper2020'\n","fmri = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg[\"decoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","num_voxel = {\n","    roi[\"name\"]: roi[\"num\"]\n","    for roi in cfg[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","label_key = cfg[\"decoder\"][\"fmri\"][\"label_key\"]\n","feature_dir_list = cfg[\"decoder\"][\"features\"][\"paths\"]\n","features = cfg[\"decoder\"][\"features\"][\"layers\"]\n","feature_index_file = cfg.decoder.features.get(\"index_file\", None)\n","decoder_dir = cfg[\"decoder\"][\"path\"]\n","decoded_feature_dir = cfg[\"decoded_feature\"][\"path\"]\n","cv_folds = cfg.cv.get(\"folds\", None)\n","cv_exclusive = cfg.cv.get(\"exclusive_key\", None)\n","cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]\n","excluded_labels = cfg.decoded_feature.fmri.get(\"exclude_labels\", [])\n","average_sample = cfg.decoded_feature.parameters.get(\"average_sample\", True)\n","\n","# Show settings\n","print(\"====== Decoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri.keys())))\n","print(\"ROIs:     {}\".format(list(rois.keys())))\n","print(\"Features: {}\".format(features))\n","print(\"Folds:    {}\".format(cv_labels))\n","print(\"\")\n","print(\"fMRI data dir:            {}\".format(os.path.split(list(fmri.values())[0][0])[0]))\n","print(\"Feature data dir:         {}\".format(feature_dir_list[0]))\n","print(\"\")\n","print(\"Save decoder dir:         {}\".format(decoder_dir))\n","print(\"Save decoded feature dir: {}\".format(decoded_feature_dir))\n","print(\"===============================\")\n"]},{"cell_type":"markdown","metadata":{},"source":["You can also check the settings used in the analysis on the paper (`./config/encoding_emotion_score_cv_paper2020.yaml`) by executing the cell below.\n","<!--実際に paper の解析で用いられている設定（`./config/decoding_emotion_score_cv_paper2020.yaml`）についても，下記のセルを実行すると確認できます．-->\n","\n","By specifying this config file, you can change the subsequent training and prediction to conform to the analysis of the paper. However, **please note that the parameters will be fixed conditions.** The phase of optimizing parameters by performing further fold calculations in the training dataset during the fold calculation of cross-validation is not implemented in the Python version. \n","<!--こちらに config file の指定を差し替えることで，以降の training や prediction について，paper の解析に準拠したものに変更することができます．**ただし，パラメータが固定された条件になることにご注意ください．**交差検証のfold計算中に，さらにtraining dataset内のfold計算を行ってパラメータの最適化を行うフェーズについては，Python 版で実装されていません．-->\n","In addition, if you calculate all conditions, the required calculation memory will increase, and it may not be possible to run it on Google Colab. \n","Moreover, the calculation time will be very long. \n","So please run it in a local machine environment. \n","<!--また，すべての条件を計算する場合は，必要な計算メモリが増大し，Google colabでは実行できない可能性があります．また，計算時間が長くなるため，ローカルマシン環境で実行するようにしてください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load config data\n","config_file = \"./config/decoding_emotion_score_cv_paper2020.yaml\"\n","config_file = Path(config_file)\n","config_name = str(config_file.stem)\n","config_dir = str(config_file.absolute().parent)\n","with initialize_config_dir(config_dir=config_dir):\n","    cfg2 = compose(config_name=config_name)    \n","\n","# Convert settings to variables\n","fmri2 = {\n","    subject[\"name\"]: subject[\"paths\"]\n","    for subject in cfg2[\"decoder\"][\"fmri\"][\"subjects\"]\n","}\n","rois2 = {\n","    roi[\"name\"]: roi[\"select\"]\n","    for roi in cfg2[\"decoder\"][\"fmri\"][\"rois\"]\n","}\n","feature_dir_list2 = cfg2[\"decoder\"][\"features\"][\"paths\"]\n","features2 = cfg2[\"decoder\"][\"features\"][\"layers\"]\n","decoder_dir2 = cfg2[\"decoder\"][\"path\"]\n","decoded_feature_dir2 = cfg2[\"decoded_feature\"][\"path\"]\n","cv_folds2 = cfg2.cv.get(\"folds\", None)\n","cv_labels2 = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds2))]\n","\n","# Show settings\n","print(\"====== Decoding settings ======\")\n","print(\"Subjects: {}\".format(list(fmri2.keys())))\n","print(\"ROIs:     {}\".format(list(rois2.keys())))\n","print(\"Features: {}\".format(features2))\n","print(\"Folds:    {}\".format(cv_labels2))\n","print(\"\")\n","print(\"fMRI data dir:            {}\".format(os.path.split(list(fmri2.values())[0][0])[0]))\n","print(\"Feature data dir:         {}\".format(feature_dir_list2[0]))\n","print(\"\")\n","print(\"Save decoder dir:         {}\".format(decoder_dir2))\n","print(\"Save decoded feature dir: {}\".format(decoded_feature_dir2))\n","print(\"===============================\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's perform the calculation with the settings of the tutorial above.\n","<!--それでは，上記の tutorial のセッティングで計算を行っていきます．-->"]},{"cell_type":"markdown","metadata":{},"source":["### Train the decoder models (~5min)\n","<!--### Decoder training を行う (~5分)-->\n","\n","In this subsection, we will train the decoder model. Please make sure that the current directory is directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--Decoderのtrainingを行います． カレントディレクトリが， `EmotionVideoNeuralRepresentationPython` の直下であることを確認してください．（`!pwd`）-->\n","\n","If running on a local machine, you can also perform training the decoder according to the specified config file by executing the command below.\n","<!--もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の training が開始されます-->\n","\n","`$ python feature-decoding/cv_train_decoder_fastl2lir.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml`\n","\n","This calculation can also be run in parallel across multiple machines. (The current directory and the result storage directory must be shared between those machines).\n","Parallel execution on multiple machines can significantly reduce the calculation time.\n","<!--この計算は，異なるマシン上で並行して実行することが可能です（カレントディレクトリ，および保存先ディレクトリをマシン間で共有している必要があります）．\n","特に，全ての条件を計算する場合，複数マシンでの並列実行を推奨します．-->\n","\n","In this tutorial, we will directly call and execute the main function called by the above script `cv_train_decoder_fastl2lir.py`.\n","<!--ここでは，　上記のスクリプト `cv_train_decoder_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from cv_train_decoder_fastl2lir import featdec_cv_fastl2lir_train\n","\n","print(fmri)\n","\n","featdec_cv_fastl2lir_train(\n","    fmri,\n","    feature_dir_list,\n","    output_dir=decoder_dir,\n","    rois=rois,\n","    num_voxel=num_voxel,\n","    label_key=label_key,\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    layers=features,\n","    feature_index_file=feature_index_file,\n","    alpha=cfg[\"decoder\"][\"parameters\"][\"alpha\"],\n","    chunk_axis=cfg[\"decoder\"][\"parameters\"][\"chunk_axis\"],\n","    analysis_name=analysis_name\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["The trained decoder model will be output to the following directory.\n","<!--下記のディレクトリに訓練済み decoder model が出力されます．-->\n","\n","`<Save decoder dir>/<feature>/<subject>/<ROI>/<Fold>/model/`\n","\n","e.g.) `./data/feature_decoders/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/model/`\n","\n","- `W.mat`: Weight of encoder model\n","- `b.mat`: Bias of encoder model\n","- `x_mean.mat`, `x_norm.mat`: Voxel normalization parameters. \n","- `y_mean.mat`, `y_norm.mat`: Feature normalization parameters."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls ./data/feature_decoders/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/model\n"]},{"cell_type":"markdown","metadata":{},"source":["After calculating, please run the script below to check that all specified conditions are complete. Be sure to run it if an error occurs during execution or if you are performing parallel calculations.\n","<!--計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．実行途中でエラーが発生したり，並列計算したりした場合は，必ず実行してください．-->\n","\n","`$ python src/check_training_prediction.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml --override +wash=train`\n","\n","Here, directly call and execute the main function called by the above script `check_training_prediction.py`. There is no problem if you get the output below.\n","<!--ここでは，　上記のスクリプト `check_training_prediction.py` で呼び出されるメイン関数を直接呼び出し，実行します．\n","下記が出力されれば問題ありません．-->\n","\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","```\n","\n","If you are informed that there are unfinished conditions, run the above cell (command) again to ensure that all conditions are completed.\n","<!--もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"train\"\n","analysis_type = \"decoding\"\n","\n","check_training_prediction(\n","    decoder_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["This completes decoder training.\n","<!--これで encoder の training は完了です．-->"]},{"cell_type":"markdown","metadata":{},"source":["### Predict emotional scores by using the trained decoder models (~5min)\n","<!--### Decoding prediction を行う（~5分）-->\n","In this subsection, we will perform prediction using the encoder model trained in the previous section. Please make sure that the current directory is directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--このセクションでは，前セクションでtrainingしたdecoder modelを用いて，predictionを行います．カレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください．-->\n","\n","If running on a local machine, you can also perform predictions according to the specified config file by executing the command below. This calculation can also be performed in parallel across multiple machines. \n","<!--もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて encoder の prediction が開始されます-->\n","<!--この計算も，並列実行することが可能です．-->\n","\n","`$ python feature-decoding/cv_predict_feature_fastl2lir.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml`\n","\n","In this tutorial, we will directly call and execute the main function called by the above script `cv_predict_feature_fastl2lir.py`.\n","<!--ここでは，　上記のスクリプト `cv_predict_feature_fastl2lir.py` で呼び出されるメイン関数を直接呼び出し，実行します．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from cv_predict_feature_fastl2lir import featdec_cv_fastl2lir_predict\n","\n","featdec_cv_fastl2lir_predict(\n","    fmri,\n","    decoder_dir,\n","    output_dir=decoded_feature_dir,\n","    rois=rois,\n","    label_key=label_key,\n","    cv_key=cfg[\"cv\"][\"key\"],\n","    cv_folds=cv_folds,\n","    cv_exclusive=cv_exclusive,\n","    layers=features,\n","    feature_index_file=feature_index_file,\n","    excluded_labels=excluded_labels,\n","    average_sample=average_sample,\n","    chunk_axis=cfg[\"decoder\"][\"parameters\"][\"chunk_axis\"],\n","    analysis_name=analysis_name\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The predicted emotional scores for each stimulus will be output to the following directory.\n","<!--下記のディレクトリに推定された emotional scores が各刺激ごとに出力されます．-->\n","\n","`<Save decoded feature dir>/<feature>/<subject>/<ROI>/<Fold>/decoded_features/`\n","\n","e.g.) `./data/decoded_features/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/decoded_features/`\n","\n","- `*.mat`: Predicted scores corresponding to each stimulus\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls ./data/decoded_features/decoding_emotion_score_dimension_cv_paper2020_voxel500_alpha100/amt/mean_score_concat/category/Subject1/PVC/cv-fold1/decoded_features/"]},{"cell_type":"markdown","metadata":{},"source":["After calculating, please run the script below to check that all specified conditions are complete. Be sure to run it if an error occurs during execution or if you are performing parallel calculations.\n","<!--計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．-->\n","\n","`$ python src/check_training_prediction.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml --override +wash=predict`\n","\n","Here, directly call and execute the main function called by the above script `check_training_prediction.py`. There is no problem if you get the following two outputs.\n","<!-- ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します．下記の2つの結果が出力されることを確認してください． -->\n","```\n","==========\n","No processes were terminated during execution.\n","==========\n","\n","==========\n","All predicted files exist.\n","==========\n","```\n","\n","If you are informed that there are unfinished conditions, run the above cell (command) again to ensure that all conditions are completed.\n","<!--もし，未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．-->\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from check_training_prediction import check_training_prediction\n","\n","target = \"predict\"\n","analysis_type = \"decoding\"\n","\n","check_training_prediction(\n","    decoded_feature_dir,\n","    target,\n","    analysis_type,\n","    analysis_name,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["This completes the prediction of the decoding.\n","<!--これで decoder の prediction は完了です．-->"]},{"cell_type":"markdown","metadata":{},"source":["### Calculate the accuracy of prediction (~5min)\n","<!--### Decoding prediction の performance の計算 （~5分）-->\n","\n","In this sub section, we will calculate the accuracy of the predicted scores. Specifically, we calculate the correlation coefficient for each category between the emotional scores predicted in the previous section and the averaged emotional scores obtained through crowdsourcing.\n","The current directory should be directly under `EmotionVideoNeuralRepresentationPython`.\n","<!--Decoding結果の評価を行います．具体的には，前セクションで推定されたemotional scoreと，クラウドソーシングで得られた平均scoreについて，categoryごとにcorrelation coefficientを計算します．\n","カレントディレクトリは， `EmotionVideoNeuralRepresentationPython` の直下としてください．-->\n","\n","If running on a local machine, you can also evaluate the predicted scores according to the specified config file by executing the command below. This calculation can also be performed in parallel across multiple machines.\n","<!--もし，ローカル環境で実行している場合，下記の通りコマンドを実行することで，上記の設定ファイルに準じて推定scoreの評価が開始されます-->\n","<!--この計算は，並列実行することが可能です．デモでは不要ですが，全ての条件を計算する場合，複数台のマシンでの並列実行を推奨します．\n","-->\n","\n","`$ python src/cv_evaluate_predicted_features.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml `\n","\n","In this tutorial, we will directly call and execute the main function called by the above script `cv_evaluate_predicted_features.py`.\n","<!--ここでは，　上記のスクリプト `cv_evaluate_predicted_features.py` で呼び出されるメイン関数を直接呼び出し，実行します．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from cv_evaluate_predicted_features import cv_evaluate_predicted_features\n","\n","cv_evaluate_predicted_features(\n","    decoded_feature_dir,\n","    feature_dir_list[0],\n","    output_file_pooled=os.path.join(decoded_feature_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(decoded_feature_dir, 'evaluation_fold.db'),\n","    subjects=fmri,\n","    rois=rois,\n","    layers=features,\n","    cv_folds=cv_folds,\n","    feature_index_file=feature_index_file,\n","    feature_decoder_path=decoder_dir,\n","    average_sample=average_sample,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The `sqlite3` database file containing the evaluation results will be saved in the following directory. For information on how to access the accuracy values, please refer to the description of `Read accuracy` in the code in the next section.\n","<!--下記のディレクトリに評価結果を格納した `sqlite3` のデータベースファイルが保存されます．アクセス方法については，次セクションにおけるコード中の`Read accuracy`の記述を参考にしてください．-->\n","\n","`<Save decoded feature dir>/evaluation.db`\n","\n","e.g.) `./data/decoded_features/encoding_emotion_score_cv_paper2020_featall_alpha100/amt/mean_score_concat/evaluation.db`\n"]},{"cell_type":"markdown","metadata":{},"source":["After calculating, please run the script below to check that all specified conditions are complete. Be sure to run it if an error occurs during execution or if you are performing parallel calculations.\n","<!--計算終了後， 下記のスクリプトを実行することで， 全ての計算が完了していることを確認します．-->\n","\n","`$ python src/check_evaluate_database.py config/tutorial_decoding_emotion_score_cv_paper2020.yaml `\n","\n","Here, directly call and execute the main function called by the above script `check_evaluate_database.py`. Please confirm that `All conditions are finished` is printed twice. Once indicates that each fold evaluation has been completed, and the other indicates that the pooled evaluation by averaging the fold results, has been completed.\n","<!--ここでは，　上記のスクリプトで呼び出されるメイン関数を直接呼び出し，実行します． `All conditions are finished` が二度出力されていることを確認してください．一つは各foldの計算が完了していることを示しており，もう一つはfoldの計算結果を平均したpooledの計算が完了していることを示しています．-->\n","```\n","==========\n","All conditions are finished.\n","==========\n","\n","==========\n","All conditions are finished.\n","==========\n","```\n","\n","If you are informed that there are unfinished conditions, run the above cell (command) again to ensure that all conditions are completed.\n","<!--もし，　未終了の条件があると指示された場合は，再度，上記のセル（コマンド）を実行し，すべての条件を確実に完了させてください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from check_evaluate_database import check_evaluate_database\n","\n","analysis_type = \"decoding\"\n","\n","check_evaluate_database(\n","    decoded_feature_dir,\n","    analysis_type,\n","    list(fmri.keys()),\n","    list(rois.keys()),\n","    features,\n","    cv_folds,\n","    output_file_pooled=os.path.join(decoded_feature_dir, 'evaluation.db'),\n","    output_file_fold=os.path.join(decoded_feature_dir, 'evaluation_fold.db'),\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["Now all the calculations for decoding analysis are complete. Finally, we will visualize it.\n","<!--これで decoding analysis の全てのステップが完了しました．最後に可視化を行います．-->"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize decoding performance (~10min)\n","<!--### Decoding accuracy の可視化 （~10分）-->\n","\n","In this sub section, we visualize the decoding accuracy calculated in the previous section.\n","<!--ここでは前セクションで計算されたcorrelation coefficient，すなわちdecoding accuracyを可視化します．-->\n","\n","First, we plot the accuracy of each category using a swarmplot with each grouping ROI as a point. This corresponds to a simplified version of Fig. 2A in our paper. Please note that the results does not match exactly because the calculation conditions are reduced.\n","<!--まずは，各カテゴリ の accuracy を各grouping ROIをpointとしたswarmplot で描画します．\n","これは本稿の Fig.2A の簡易版に該当します．計算条件が緩和されているため，厳密一致しないことには注意してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig. 2A Decoding accuracy for individual categories.\n","import pandas as pd\n","import seaborn as sns\n","\n","category_index_list = {\n","    30: 'sexual_desire', \n","    2: 'aesthetic_appreciation', \n","    16: 'entrancement', \n","    14: 'disgust', \n","    3: 'amusement', \n","    19: 'fear', \n","    5: 'anxiety', \n","    22: 'interest', \n","    31: 'surprise', \n","    23: 'joy', \n","    21: 'horror', \n","    1: 'adoration', \n","    9: 'calmness', \n","    27: 'romance', \n","    6: 'awe', \n","    24: 'nostalgia', \n","    12: 'craving', \n","    15: 'empathic_pain', \n","    26: 'relief', \n","    7: 'awkwardness', \n","    18: 'excitement', \n","    28: 'sadness', \n","    8: 'boredom', \n","    33: 'triumph',    \n","    0: 'admiration',\n","    29: 'satisfaction', \n","    32: 'sympathy', \n","    4: 'anger', \n","    10: 'confusion', \n","    13: 'disappointment', \n","    25: 'pride', \n","    17: 'envy', \n","    11: 'contempt', \n","    20: 'guilt'\n","}\n","\n","# Connect to accuracy database\n","evaluation_filepath = os.path.join(decoded_feature_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","dataframe = {\"subject\": [], \"roi\": [], \"category\": [], \"accuracy\": []}\n","for subject, roi, (cat_i, category) in product(fmri.keys(), rois.keys(), category_index_list.items()):                \n","    # Read accuracy \n","    acc = db.get(subject=subject, roi=roi, layer=feature, metric=metric)[cat_i]\n","    dataframe[\"subject\"].append(subject)\n","    dataframe[\"roi\"].append(roi)\n","    dataframe[\"category\"].append(category)\n","    dataframe[\"accuracy\"].append(acc)\n","    \n","dataframe = pd.DataFrame.from_dict(dataframe)\n","display(dataframe)\n","\n","fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n","ax.axhline(0, c=\"k\")\n","for i in [0.2, 0.4, 0.6]:\n","    ax.axhline(i, c=\"lightgray\")\n","sns.swarmplot(data=dataframe, x=\"category\", y=\"accuracy\", ax=ax)\n","ax.set_xticklabels(ax.get_xticklabels(),rotation = 45, ha=\"right\")\n","ax.text(29, 0.4, \"1 subject\\nn=22\", fontsize=20)\n","ax.set_title(\"Emotion category\", fontsize=20)\n","ax.spines['right'].set_visible(False)\n","ax.spines['top'].set_visible(False)\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, we use `pycortex` to map the accuracy of each ROI onto the flattend cortex of the subject.\n","<!--次に `pycortex`を使用して，各ROIのcorrelation coefficientを被験者の展開された皮質上にマップします．-->\n","\n","Before doing this, please check the following two points.\n","<!--まずは下記の2点を確認してください．-->\n","\n","1. The `filestore` of `pycortex` is `./data/pycortex`.\n","2. `INKSCAPE_VERSION` is not `None`.\n","\n","If the conditions are not met, please check the Setup section.\n","<!--条件が満たされていない場合，Setup sectionを確認してください．-->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cortex\n","print('pycortex filestore:', cortex.config['basic']['filestore'])\n","print('INKSCAPE_VERSION:', cortex.testing_utils.INKSCAPE_VERSION)\n"]},{"cell_type":"markdown","metadata":{},"source":["If the above conditions are met, run the cell below to visualize the results.\n","The decoding accuracy for each ROI is displayed on a cortical map for the three categories \"Sexual desire,\" \"Aesthetic appreciation,\" and \"Entrancement.\"\n","<!--上記の条件が満たされているならば，下記のセルを実行して，結果の可視化を行ってください．ここでは，\"Sexual desire\"，\"Aesthetic appreciation\"，および\"Entrancement\"の3つのカテゴリについて，ROIごとのdecoding accuracyが皮質マップ上に表示されます．-->\n","\n","This corresponds to a simplified version of Fig. 2D in our paper. Please note that the results does not match exactly because the calculation conditions are reduced.\n","<!--これは本稿の Fig.2D の簡易版に該当します．計算条件が緩和されているため，厳密一致しないことには注意してください．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fig.4A Cortical surface maps of decoding accuracies for individual categories\n","\n","pycortex_align = 'voxel2mm_fmriprep'\n","pycortex_overlay_filepath = './data/pycortex/{}/overlays.svg'\n","metric = \"profile_correlation\"\n","vmin = 0.0\n","vmax = 100.0\n","feature = \"category\"\n","category_index_list = {\n","    30: 'sexual_desire', \n","    2: 'aesthetic_appreciation', \n","    16: 'entrancement', \n","}\n","\n","# Connect to accuracy database\n","evaluation_filepath = os.path.join(decoded_feature_dir, \"evaluation.db\")\n","db = SQLite3KeyValueStore(evaluation_filepath)\n","\n","for subject in fmri.keys():\n","    # Load Bdata\n","    bdata = bdpy.BData(fmri[subject][0])\n","                \n","    for cat_i, category in category_index_list.items():\n","        print(\"Category:\", category)\n","        \n","        # Create emty volume\n","        # Colormap (cmap) for visualization is set here. Available cmap is same as Matplotlib \n","        # (See document for matplotlib.pylab.colormaps()).\n","        voxel_vol = cortex.Volume.empty(subject, \n","                                        pycortex_align, \n","                                        value=np.nan,\n","                                        cmap='RdGy_r')\n","\n","        # Get category accuracy & min-max normalization\n","        cat_acc = []\n","        for roi in rois.keys():\n","            # Read accuracy \n","            acc = db.get(subject=subject, roi=roi, layer=feature, metric=metric) \n","            cat_acc.append(acc[cat_i])\n","        cat_acc = np.array(cat_acc)\n","        \n","        # Min-max normalize\n","        cat_acc = (cat_acc - np.nanmin(cat_acc)) / (np.nanmax(cat_acc) - np.nanmin(cat_acc)) * 100\n","        \n","        for roi_i, roi in enumerate(rois.keys()):\n","            # Load voxel information\n","            _, voxel_selector = bdata.select(rois[roi], return_index = True)\n","\n","            # Apply voxel selector to voxel_i/j/k array\n","            voxel_ijk = np.vstack([\n","                bdata.get_metadata('voxel_i')[voxel_selector].astype('int32'),\n","                bdata.get_metadata('voxel_j')[voxel_selector].astype('int32'),\n","                bdata.get_metadata('voxel_k')[voxel_selector].astype('int32'),\n","            ]).astype(int)\n","\n","            \n","            # Mapped to each voxel\n","            voxel_vol.data[ voxel_ijk[2], voxel_ijk[1], voxel_ijk[0] ] = cat_acc[roi_i]\n","\n","        # Set vmax and vmin for visualization                    \n","        voxel_vol.vmax = vmax\n","        voxel_vol.vmin = vmin\n","\n","        # Set text\n","        text_posi_x = 0.5\n","        text_posi_y = 0.9\n","        show_text = category\n","        \n","        # Show\n","        fig = cortex.quickshow( \n","                    voxel_vol, with_colorbar=True, with_curvature=True, \n","                    overlay_file=pycortex_overlay_filepath.format(subject),\n","                    labelsize=50, linewidth=3,\n","            )\n","        fig.text(text_posi_x, text_posi_y, show_text, \n","                    horizontalalignment='center', verticalalignment='center', size=50)\n","        plt.show()\n","        "]},{"cell_type":"markdown","metadata":{},"source":["This completes the decoding analysis tutorial. Congratulations on your hard work!\n","<!--以上でdecoding analysisのtutorialは終了です．お疲れ様でした．-->\n","\n","If you would like to know more about the encoding calculation procedure, please refer to `01_decoding.ipynb`(In progress) in the [feature-decoding](https://github.com/KamitaniLab/feature-decoding) repository.\n","<!--Decoding のより詳細な計算手続きを知りたい場合は，　[feature-decoding](https://github.com/KamitaniLab/feature-decoding) リポジトリ内の handson を参照してください．-->\n","\n","<br/>\n","<br/>"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
